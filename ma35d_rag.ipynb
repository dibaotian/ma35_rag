{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 数量： 2\n",
      "GPU 0: Tesla V100-PCIE-16GB，内存使用情况：3748.625 MB / 16384.0 MB\n",
      "GPU 1: Tesla V100-PCIE-16GB，内存使用情况：1158.625 MB / 16384.0 MB\n"
     ]
    }
   ],
   "source": [
    "# %pip install nvidia-ml-py3\n",
    "import pynvml\n",
    "\n",
    "# 初始化 NVML 库\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 获取 GPU 数量\n",
    "num_gpus = pynvml.nvmlDeviceGetCount()\n",
    "print(\"GPU 数量：\", num_gpus)\n",
    "\n",
    "# 遍历每个 GPU，获取其资源信息\n",
    "for i in range(num_gpus):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    gpu_memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU {i}: {gpu_name.decode()}，内存使用情况：{gpu_memory_info.used / 1024 / 1024} MB / {gpu_memory_info.total / 1024 / 1024} MB\")\n",
    "\n",
    "# 关闭 NVML 库\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# LLamaIndex 使用 PyTorch 进行向量计算\n",
    "# 清理GPU 资源\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 数量： 2\n",
      "GPU 0: Tesla V100-PCIE-16GB，内存使用情况：3748.625 MB / 16384.0 MB\n",
      "GPU 1: Tesla V100-PCIE-16GB，内存使用情况：1158.625 MB / 16384.0 MB\n"
     ]
    }
   ],
   "source": [
    "# 初始化 NVML 库\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 获取 GPU 数量\n",
    "num_gpus = pynvml.nvmlDeviceGetCount()\n",
    "print(\"GPU 数量：\", num_gpus)\n",
    "\n",
    "# 遍历每个 GPU，获取其资源信息\n",
    "for i in range(num_gpus):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    gpu_memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU {i}: {gpu_name.decode()}，内存使用情况：{gpu_memory_info.used / 1024 / 1024} MB / {gpu_memory_info.total / 1024 / 1024} MB\")\n",
    "\n",
    "# 关闭 NVML 库\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "04/19/2024 15:04:06 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: maidalun1020/bce-embedding-base_v1\n",
      "/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "04/19/2024 15:04:12 - [INFO] -sentence_transformers.SentenceTransformer->>>    2 prompts are loaded, with the keys: ['query', 'text']\n",
      "04/19/2024 15:04:15 - [INFO] -BCEmbedding.models.RerankerModel->>>    Loading from `maidalun1020/bce-reranker-base_v1`.\n",
      "04/19/2024 15:04:15 - [INFO] -BCEmbedding.models.RerankerModel->>>    Execute device: cuda;\t gpu num: 2;\t use fp16: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %pip install llama-index-llms-ollama\n",
    "# !pip install llama-index\n",
    "# %pip install llama-index-embeddings-ollama\n",
    "# %pip install docx2txt\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-instructor\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# load the ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from BCEmbedding.tools.llama_index import BCERerank\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# model set\n",
    "llama2_7b = \"llama2\"\n",
    "llama2_13b = \"llama2:13b\"\n",
    "llama3_8b = \"llama3\"\n",
    "llama3_70b = \"llama3:70b\"\n",
    "\n",
    "# connect with the ollama server, and talk with llama2 \n",
    "\n",
    "llm_llama = Ollama(model=llama3_8b, request_timeout=600, temperature=0.1, device='cuda') #base_url = 'http://localhost:11434',\n",
    "# llm_llama2 = Ollama(model=\"llama2:13b\", request_timeout=600, temperature=0.1) #base_url = 'http://localhost:11434',\n",
    "# embedding_model = OllamaEmbedding(model_name=\"llama2\",ollama_additional_kwargs={\"mirostat\": 0}) #base_url=\"http://localhost:11434\"\n",
    "\n",
    "embed_args = {'model_name': 'maidalun1020/bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 256, 'device': 'cuda'}\n",
    "embedding_model = HuggingFaceEmbedding(**embed_args)\n",
    "\n",
    "reranker_args = {'model': 'maidalun1020/bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda'}\n",
    "reranker_model = BCERerank(**reranker_args)\n",
    "\n",
    "Settings.llm = llm_llama\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "# Settings.num_output = 512\n",
    "# Settings.context_window = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:04:16 - [INFO] -chromadb.telemetry.product.posthog->>>    Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# create a vector storage\n",
    "# %pip install llama-index-vector-stores-chroma\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "chroma_client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80779af065c146828f4cd9e0a2e1d2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459ef38d1fca4d4ba3477d846baba260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5c784da5574435b3940338379685ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d31655b38534ecba8dc21e5e4c75b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcce4e3e78e4cbbbc61e8ee2ac3a4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf21fad3b3cc4a40911dfa9d32921951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %pip install llama-index-readers-web\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/index.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/getting_started_on_prem.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/virtualization.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/tutorials.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/quality_analysis.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/filters.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/tutorials.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/filters.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xcompositor.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xabrladder.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/xma/xma_apps.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/specs_and_features.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/package_feed.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/using_ffmpeg.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/using_gstreamer.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/unified_logging.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/tuning_video_quality.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/tuning_pipeline_latency.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/managing_compute_resources.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/c_apis.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/card_management.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/encoder_comp_matrix.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-resampler.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-devices.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-all.html\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/H.264\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/H.265\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/AV1\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Scaling\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Null\",\n",
    "     \"https://trac.ffmpeg.org/wiki/FilteringGuide\",\n",
    "     ]\n",
    "     \n",
    ")\n",
    "\n",
    "collection_name = \"ma35_rag_base_beg\"\n",
    "collection = chroma_client.list_collections()\n",
    "if collection_name in collection:\n",
    "    chroma_client.delete_collection(collection_name)\n",
    "    chroma_client.clear_system_cache()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(docstore=documents, vector_store=vector_store)\n",
    "\n",
    "# 这个nodes 有什么用处\n",
    "from llama_index.core.node_parser import SimpleNodeParser \n",
    "# Initialize the parser \n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=500, chunk_overlap=20) \n",
    "# Parse documents into nodes \n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "# print(nodes[0])\n",
    "len(nodes)\n",
    "\n",
    "# %pip install ipywidgets\n",
    "# index = VectorStoreIndex.from_documents(documents,storage_context=storage_context,show_progress=True)\n",
    "index = VectorStoreIndex(nodes,embed_model=embedding_model,storage_context=storage_context,show_progress=True)\n",
    "\n",
    "# # documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # 读取 HTML 文件\n",
    "# with open(\"local_html/FFMPEG command line arguments - VideoDC - Xilinx Enterprise Wiki.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     html_content = file.read()\n",
    "\n",
    "# # 使用 Beautiful Soup 解析 HTML\n",
    "# soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# # 提取文档内容\n",
    "# document_text = soup.get_text()\n",
    "\n",
    "# # 打印文档内容\n",
    "# print(document_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your index from stored vectors\n",
    "\n",
    "collection_name = \"ma35_rag_base\"\n",
    "collection = chroma_client.list_collections()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, embed_model=embedding_model,storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = index.as_retriever()\n",
    "# relevant_docs = retriever.retrieve(\"what is the max transcode rate for 1080p30 stream\")\n",
    "# relevant_docs\n",
    "\n",
    "# \"\"\"\n",
    "# response_mode\n",
    "\n",
    "#     REFINE = \"refine\"\n",
    "#     COMPACT = \"compact\"\n",
    "#     SIMPLE_SUMMARIZE = \"simple_summarize\"\n",
    "#     TREE_SUMMARIZE = \"tree_summarize\"\n",
    "#     GENERATION = \"generation\"\n",
    "#     NO_TEXT = \"no_text\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# ResponseMode为tree_summarize时，LLM会对每一段文本进行最大长度的分割，并进行连续的读取和询问。这种模式的优点是可以保证对文本的完整理解和回答，但如果没有正确处理分割段落的情况，可能会导致错误的生成结果\n",
    "# ResponseMode为generation时，生成的回答不依赖于文档的内容，只基于提供的问题进行生成。这种模式适用于纯粹的问题回\n",
    "# ResponseMode为no_text时，生成的回答中不包含任何内容，仅作为占位符使用\n",
    "# ResponseMode为simple_summarize时，LLM会截取每段文本的相关句子（通常是第一句），并进行提炼生成回答。这种模式适用于对结果要求不高的场景。\n",
    "# ResponseMode为refine时，如果只有一个文本块（text_chunk），则会正常生成回答。但如果存在多个文本块，则会以类似轮询的方式迭代生成回答。这种模式可以对多个文本块进行迭代式的回答生成，逐步完善回答内容。\n",
    "# ResponseMode为compact时，生成的回答会将多个文本块（text_chunk）压缩到设定的最大长度，并生成一次回答。然后，根据后续内容对以往的答案进行改进和完善（即进行多次迭代）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine = index.as_query_engine(response_mode='simple_summarize')\n",
    "\n",
    "# template = (\n",
    "#     \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "#     \"Context information from multiple sources is below.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "#     \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "#     \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "#     \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "#     \"Answers need to be precise and concise.\\n\"\n",
    "#     \"Query: {query_str}\\n\"\n",
    "#     \"Answer: \"\n",
    "# )\n",
    "# qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "# query_engine.update_prompts(\n",
    "#     {\"response_synthesizer:text_qa_template\": qa_template}\n",
    "# )\n",
    "\n",
    "\n",
    "# template = (\n",
    "#     \"The original query is as follows: {query_str}.\\n\"\n",
    "#     \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "#     \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "#     \"-------------\\n\"\n",
    "#     \"{context_msg}\\n\"\n",
    "#     \"-------------\\n\"\n",
    "#     \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "#     \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "#     \"Answers need to be precise and concise.\\n\"\n",
    "#     \"Refined Answer: \"\n",
    "# )\n",
    "\n",
    "\n",
    "# qa_template = PromptTemplate(template)\n",
    "\n",
    "# query_engine.update_prompts(\n",
    "#     {\"response_synthesizer:refine_template\": qa_template}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15566df9ede647acbe712f75e3ad50bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command\n",
      "\n",
      "   \n",
      "   \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \n",
      "Answer:Empty Response\n",
      " \n",
      "source_nodes length:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here no database input, so no answer\n",
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command\\n\n",
    "   \n",
    "   \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \"\"\",\n",
    "\n",
    "   # \"\"\"\n",
    "   # explain following ffmpeg command\\n\n",
    "\n",
    "   # ffmpeg -y -hwaccel ama \\\n",
    "   #    -c:v h264_ama  -out_fmt nv12 -i <INPUT>  \\\n",
    "   #    -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; \\\n",
    "   #                   [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" \\\n",
    "   #    -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv \\\n",
    "   #    -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv  \\\n",
    "   #    -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv \\\n",
    "   #    -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
    "   # \"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "   print(\" \")\n",
    "\n",
    "   \n",
    "   print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "      print(result)\n",
    "\n",
    "      # print(f\"Result {i}: Document ID {result['id']}, Title '{result['title']}', Similarity: {result.score}\")\n",
    "      print(f\"Result {i}\\n Similarity: {result.score}\\n content '{result.get_content()}\")\n",
    "\n",
    "   # print()\n",
    "   # # print(response.get_formatted_sources(length=10))\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"ma35_rag_base_beg\"\n",
    "collection = chroma_client.list_collections()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, embed_model=embedding_model,storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_beg = index.as_query_engine(response_mode='refine',similarity_top_k=50, temperature=0.6,node_postprocessors=[reranker_model])\n",
    "\n",
    "template = (\n",
    "    \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"you must answer every question in chinese\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine_beg.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine_beg.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d377f6ed014e90b69351b4b396840c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/19/2024 15:05:25 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:05:29 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:05:34 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:05:39 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:05:44 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command 'ffmpeg -hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama -i infile.mp4 -c:v hevc_ama -b:v 1000K -r 60 -f mp4 -y transcoded.mp4' \n",
      "Answer:I'm happy to refine the original answer!\n",
      "\n",
      "The FFmpeg command `ffmpeg -hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama -i infile.mp4 -c:v hevc_ama -b:v 1000K -r 60 -f mp4 -y transcoded.mp4` is used for transcoding a video file using AMA (AmpliFi Media Accelerator) hardware acceleration.\n",
      "\n",
      "Here's a breakdown of the command:\n",
      "\n",
      "* `ffmpeg`: The FFmpeg application, which uses AMD's AMA Video SDK.\n",
      "* `-hwaccel ama`: Enable AMA hardware acceleration.\n",
      "* `-hwaccel_device /dev/ama_transcoder0`: Specify the AMA device as `/dev/ama_transcoder0`.\n",
      "* `-c:v h264_ama` and `-c:v hevc_ama`: Encode the video using H.264 and HEVC, respectively, with AMA hardware acceleration.\n",
      "* `-i infile.mp4`: Input file is `infile.mp4`.\n",
      "* `-b:v 1000K`: Set the video bitrate to 1,000 KB (approximately 5 Mbps).\n",
      "* `-r 60`: Set the frame rate to 60 fps.\n",
      "* `-f mp4`: Output file format is MP4.\n",
      "* `-y`: Override output file if it already exists.\n",
      "* `transcoded.mp4`: Output file name is `transcoded.mp4`.\n",
      "\n",
      "In summary, this command transcodes the input video file `infile.mp4` into H.264 and HEVC encoded videos using AMA hardware acceleration and saves them as `transcoded.mp4`.\n",
      "\n",
      "The additional context provided helps to clarify the specific use case of transcoding a RAW 10-bit YUV420 format video clip using FFmpeg with AMA hardware acceleration.\n",
      "\n",
      "Refined Answer:\n",
      "\n",
      "source_nodes length:5\n",
      "Node ID: d727e5b0-17ad-4307-a99e-1e9bca50cfde\n",
      "Text: See FFmpeg's `overlay` filter for detail usage.  ## Crop and\n",
      "Shift¶  The following example demonstrates how to crop a decoded\n",
      "video, at a given x and y offsets, through host CPU, and to encode the\n",
      "resultant stream on a target AMA device:                ffmpeg\n",
      "-hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama\n",
      "-out_fmt yuv420p -i <I...\n",
      "Score:  0.579\n",
      "\n",
      "Node ID: bd395e91-af13-4939-9348-2d3b8b0ff483\n",
      "Text: See [FFmpeg Video Quality](quality_analysis.html) for details.\n",
      "**Command Line** :                ffmpeg -re -hwaccel ama -f rawvideo\n",
      "-s 1920x1080 -framerate 60 -i <INPUT> -vf \"hwupload\" -c:v av1_ama -b:v\n",
      "5M -f mp4 -y sn1_av1.mp4       Explanation of the flags:    * `-re`\n",
      "* Flag to maintain the target frame rate    * `-s 1920x1080`      *\n",
      "S...\n",
      "Score:  0.571\n",
      "\n",
      "Node ID: 09b1695b-558a-40f1-9daa-cb9b7d507bb6\n",
      "Text: ##### High Quality Encoding¶  **Command Line** :\n",
      "ffmpeg -re -hwaccel ama -f rawvideo -s 1920x1080 -framerate 60 -i\n",
      "<INPUT> -vf \"hwupload\" -c:v av1_ama -crf 1 -qp 0 -f mp4 sn1_crf_hq.mp4\n",
      "Explanation of the flags:    * `-crf 1`      * Enables the\n",
      "[`-crf`](../../using_ffmpeg.html#cmdoption-crf) mode    * `-qp 0`\n",
      "* Sets the...\n",
      "Score:  0.561\n",
      "\n",
      "Node ID: 7d844e41-4a40-4517-9713-72d95f7ffd08\n",
      "Text: ### Decode Only¶  This example accepts a clip that is already\n",
      "encoded in H.264, and will decode the file into a RAW format and save\n",
      "it to disk.  **Command Line** :                ffmpeg -y -hwaccel ama\n",
      "-c:v h264_ama -out_fmt nv12 -i <INPUT> \\      -vf\n",
      "hwdownload,format=nv12 -f rawvideo /tmp/dec_out.nv12       Explanation\n",
      "of the flags:    * `ffmp...\n",
      "Score:  0.555\n",
      "\n",
      "Node ID: 665edb21-6aa7-4733-bb75-d1eee83398eb\n",
      "Text: **Command Line** :                ffmpeg -y -hwaccel ama -c:v\n",
      "h264_ama -out_fmt nv12 -i <INPUT> \\      -vf hwdownload,format=nv12 -f\n",
      "rawvideo /tmp/dec_out.nv12       Explanation of the flags:    *\n",
      "`ffmpeg`      * The ffmpeg application, which is provided by AMD, and\n",
      "moved to the top of the PATH when you sourced the setup.sh script    *\n",
      "`hwaccel ...\n",
      "Score:  0.550\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf7c2310724468d90d27a54244bb5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:05:49 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:05:52 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:05:57 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:02 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:11 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question2: 详细解释一下下面的命令行 'gst-launch-1.0 filesrc location=<INPUT> ! parsebin ! h264parse ! ama_h264dec ! capsfilter 'caps=video/x-raw(memory:AMAMemory),format=NV12' !  ama_download ! filesink location=/tmp/h264.nv12'\n",
      "Answer:I'll refine the original answer based on the new context.\n",
      "\n",
      "The refined answer:\n",
      "\n",
      "This command line is used to decode H.264 streams, transcode them into multiple resolutions, and encode them in different formats. Here's a step-by-step explanation:\n",
      "\n",
      "* `gst-launch-1.0`: The GStreamer application.\n",
      "* `filesrc location=<INPUT>`: Reads the input file location.\n",
      "* `parsebin`: Unpacks the container to elementary stream.\n",
      "* `h264parse`: Parses H.264 streams.\n",
      "* `ama_h264dec`: Decodes H.264 streams, outputting NV12 format images.\n",
      "* `capsfilter 'caps=video/x-raw(memory:AMAMemory),format=NV12'`: Specifies the output capabilities, including NV12 format and memory storage.\n",
      "* `ama_download`: Downloads the decoded image.\n",
      "* `filesink location=/tmp/h264.nv12`: Writes the downloaded image to `/tmp/h264.nv12` file.\n",
      "\n",
      "The command line also includes additional elements for transcoding:\n",
      "\n",
      "* `ama_scaler name=s`: Scales the video stream to different resolutions.\n",
      "* `ama_h265enc`, `ama_av1enc`, and `ama_h264enc`: Encode the scaled video streams in H.265, AV1, and H.264 formats, respectively.\n",
      "* `queue` and `filesink`: Buffer and write the encoded streams to files.\n",
      "\n",
      "Note that this command line is optimized for performance by using a RAM disk (`/dev/shm`) instead of the default disk storage. This can help mitigate potential issues with disk speeds affecting FPS. The `fakesink` plug-in can also be used to display performance numbers without writing outputs to disk.\n",
      "\n",
      "To use H.265 input streams, update the scripts to use the `h265parse` GStreamer plug-in instead of `h264parse`.\n",
      "\n",
      "In addition, there are two more command lines provided:\n",
      "\n",
      "### Encode only¶\n",
      "\n",
      "* Command Line: `gst-launch-1.0 filesrc location=<INPUT> ! rawvideoparse width=1920 height=1080 format=i420 framerate=60/1 ! ama_upload ! ama_h264enc ! h264parse ! filesink location=/tmp/out.h264`\n",
      "\n",
      "This command line is used to encode a video stream in H.264 format.\n",
      "\n",
      "### Transcode with Multiple-Resolution outputs¶\n",
      "\n",
      "* Command Line: `gst-launch-1.0 filesrc location=<INPUT> ! parsebin ! ...`\n",
      "\n",
      "This command line is used to transcode a video stream into multiple resolutions and formats, including H.265, AV1, and H.264.\n",
      "\n",
      "In both cases, the GStreamer application (`gst-launch-1.0`) is used to launch the pipeline, which consists of various elements that perform tasks such as reading input files, parsing streams, decoding video, scaling, encoding, and writing output files.\n",
      "\n",
      "source_nodes length:5\n",
      "Node ID: 50ab04ff-3937-43cf-b41c-4d41c6a8a23b\n",
      "Text: For brevity purposes, explanations of the GStreamer element\n",
      "properties are not repeated after they have been explained once. The\n",
      "detailed explanation of the each GStreamer pipeline element property\n",
      "can be obtained by using `gst- inspect-1.0 <element name>` (e.g. `gst-\n",
      "inspect-1.0 ama_av1enc`).  ### Decode only¶  **Command Line** :\n",
      "...\n",
      "Score:  0.754\n",
      "\n",
      "Node ID: b6ceb565-cdf2-4ab2-b3a7-1c7fa46609c1\n",
      "Text: There is a chance that due to the massive bandwidth required for\n",
      "operating on these RAW files, you will notice a drop in FPS; this is\n",
      "not due to the AMD AMA Video SDK but the disk speeds. We recommend\n",
      "reading/writing from `/dev/shm` which is a RAM disk.  Most of the\n",
      "scripts allow to use the fakesink plug-in which only displays\n",
      "performance number...\n",
      "Score:  0.743\n",
      "\n",
      "Node ID: 9700a58d-f0d6-4be7-971e-87fa2d9727f3\n",
      "Text: There is a chance that due to the massive bandwidth required for\n",
      "operating on these RAW files, you will notice a drop in FPS; this is\n",
      "not due to the AMD AMA Video SDK but the disk speeds. We recommend\n",
      "reading/writing from `/dev/shm` which is a RAM disk.  Most of the\n",
      "scripts allow to use the fakesink plug-in which only displays\n",
      "performance number...\n",
      "Score:  0.743\n",
      "\n",
      "Node ID: c58490df-8f97-4d6e-8d88-6d08b079de5a\n",
      "Text: * `capsfilter`      * Specifies the capabilities of the streams.\n",
      "Note that keyword `capsfilter` is optional.  ### Encode only¶\n",
      "**Command Line** :                gst-launch-1.0 filesrc\n",
      "location=<INPUT> ! rawvideoparse width=1920 height=1080 format=i420\n",
      "framerate=60/1 ! ama_upload ! ama_h264enc ! h264parse ! filesink\n",
      "location=/tmp/out.h264       ...\n",
      "Score:  0.716\n",
      "\n",
      "Node ID: 41d22aa8-9733-40f7-9764-7fd31114e9f8\n",
      "Text: Note that keyword `capsfilter` is optional.  ### Encode only¶\n",
      "**Command Line** :                gst-launch-1.0 filesrc\n",
      "location=<INPUT> ! rawvideoparse width=1920 height=1080 format=i420\n",
      "framerate=60/1 ! ama_upload ! ama_h264enc ! h264parse ! filesink\n",
      "location=/tmp/out.h264       Explanation of the pipeline elements and\n",
      "their properties:    * `...\n",
      "Score:  0.708\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189b85313ed544a086701387bf8039fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:06:13 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:15 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:18 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:23 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:27 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question3: 如何使用linux 命令行去检查MA35D设备的系统状态？给我一个例子 \n",
      "Answer:To check the system status of MA35D devices using Linux command line, you can use the `mautil` commands. Here's an example:\n",
      "\n",
      "1. Check the PCIe bus ID:\n",
      "```bash\n",
      "cat /sys/class/misc/ama_transcoder{x}/bus_id\n",
      "```\n",
      "Replace `{x}` with a number between 0 and the total number of devices minus 1.\n",
      "\n",
      "2. Check the firmware version number:\n",
      "```bash\n",
      "cat /sys/class/misc/ama_transcoder0/version_information\n",
      "```\n",
      "\n",
      "3. Use `mautil` commands to check system status and device information:\n",
      "```bash\n",
      "mautil -d [<DBDF> | all] command\n",
      "```\n",
      "For example, you can use the `examine` sub-command to get a status report of your system and devices:\n",
      "```bash\n",
      "mautil examine\n",
      "```\n",
      "\n",
      "To check the current loading of all the devices in your system, use the following command:\n",
      "\n",
      "```\n",
      "xrmadm /opt/amd/ama/ma35/scripts/list_cmd.json\n",
      "```\n",
      "This will generate a report in JSON format containing the load information for all the compute unit (CU) resources. The report contains a section for each device in the system. The device sections contain sub-sections for each of the CUs (decoder, scaler, lookahead, encoder) in that device.\n",
      "\n",
      "Note: Running certain `mautil` commands may impact the performance of video pipelines..\n",
      "\n",
      "source_nodes length:5\n",
      "Node ID: cea42e08-f7a4-4da0-8ff6-757fa22f0924\n",
      "Text: Install packages >   >         >         sudo apt -y install\n",
      "libhugetlbfs0 libboost-all-dev >         dpkg --get-selections\n",
      "'*ma35*' | awk '{system(\"sudo apt -y purge \" > $1)}' >         dpkg\n",
      "--get-selections 'amd-ama*' | awk '{system(\"sudo apt -y purge \" >\n",
      "$1)}' >         sudo apt update >         sudo apt install amd-ama-\n",
      "driver=1.1.2-* amd-ama...\n",
      "Score:  0.605\n",
      "\n",
      "Node ID: d7d717f0-8e70-4e6a-8db5-7e3724b5e7fd\n",
      "Text: ### Bus ID¶  You can look-up the PCIe bus ID of a device through\n",
      "the following command:                cat\n",
      "/sys/class/misc/ama_transcoder{x}/bus_id       , where x is a number\n",
      "between 0 to total number of devices minus 1.  Example    * Bus id of\n",
      "/sys/class/misc/ama_transcoder0 is:              $ cat\n",
      "/sys/class/misc/ama_transcoder0/bus_id       0...\n",
      "Score:  0.602\n",
      "\n",
      "Node ID: 7ac38037-9f19-433b-be8e-c3f3150b70d7\n",
      "Text: ### Bus ID¶  You can look-up the PCIe bus ID of a device through\n",
      "the following command:                cat\n",
      "/sys/class/misc/ama_transcoder{x}/bus_id       , where x is a number\n",
      "between 0 to total number of devices minus 1.  Example    * Bus id of\n",
      "/sys/class/misc/ama_transcoder0 is:              $ cat\n",
      "/sys/class/misc/ama_transcoder0/bus_id       0...\n",
      "Score:  0.601\n",
      "\n",
      "Node ID: d713ca8d-7038-4fee-8741-e714c88426de\n",
      "Text: To check the current loading of all the devices in your system,\n",
      "use the following command:                xrmadm\n",
      "/opt/amd/ama/ma35/scripts/list_cmd.json       This will generate a\n",
      "report in JSON format containing the load information for all the\n",
      "compute unit (CU) resources. The report contains a section for each\n",
      "device in the system. The device ...\n",
      "Score:  0.593\n",
      "\n",
      "Node ID: 6aef12f9-5033-4586-8b77-7d9065777e29\n",
      "Text: Info: Use --help to check cmd options to use for reports\n",
      "The last device listed has DBDF of 0000:02:00.0, which describes\n",
      "Domain 0, Bus 2, Device 00, Function 0.  ### Bus ID¶  You can look-up\n",
      "the PCIe bus ID of a device through the following command:\n",
      "cat /sys/class/misc/ama_transcoder{x}/bus_id       , where x is a\n",
      "number be...\n",
      "Score:  0.591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command 'ffmpeg -hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama -i infile.mp4 -c:v hevc_ama -b:v 1000K -r 60 -f mp4 -y transcoded.mp4' \"\"\",\n",
    "   \"\"\"详细解释一下下面的命令行 'gst-launch-1.0 filesrc location=<INPUT> ! parsebin ! h264parse ! ama_h264dec ! capsfilter 'caps=video/x-raw(memory:AMAMemory),format=NV12' !  ama_download ! filesink location=/tmp/h264.nv12'\"\"\",\n",
    "   \"\"\"如何使用linux 命令行去检查MA35D设备的系统状态？给我一个例子 \"\"\"\n",
    "\n",
    "   # \"\"\"\n",
    "   # explain following ffmpeg command\\n\n",
    "\n",
    "   # ffmpeg -y -hwaccel ama \\\n",
    "   #    -c:v h264_ama  -out_fmt nv12 -i <INPUT>  \\\n",
    "   #    -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; \\\n",
    "   #                   [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" \\\n",
    "   #    -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv \\\n",
    "   #    -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv  \\\n",
    "   #    -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv \\\n",
    "   #    -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
    "   # \"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine_beg.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "\n",
    "   # print(\"\")\n",
    "   # print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   # for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "   #    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are a Video ffmpeg & gstreamer technolodge expert.\\n\"\n",
    "    \"please answer the question in chinese\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge, please read the sources carefully.\\n\"\n",
    "    \"if the question is not releate with the RDMA, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a FPGA and RDMA expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response_mode 有几种模式可选，refine， compact, tree_summarize 等，每一种都有对应的promopt template\n",
    "query_engine_tree_summarize = index.as_query_engine(response_mode='simple_summarize', streaming=True,similary_threshold=0.1, similarity_top_k=30)\n",
    "# query_engine.update_prompts(qa_template)  \n",
    "query_engine_tree_summarize.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": qa_template}\n",
    ")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "prompts_dict = query_engine_tree_summarize.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e978a24b1b7444009001d02647a7bf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: 一个ma35D AV1 codec 能处理1080p的数据流最大到多少fps \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:11:59 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context information, it is mentioned that:\n",
      "\n",
      "\"The encode capability of each engine, 4 per device, is best described as aggregated 4kp60. This, among other things, implies that densities of 4x10p80 or rate of 1x10p240 can be achieved.\"\n",
      "\n",
      "From this statement, we can infer that the MA35D AV1 codec can handle a data stream with a resolution of 1080p at a maximum frame rate of:\n",
      "\n",
      "4 x 60 FPS = 240 FPS\n",
      "\n",
      "So, the answer is: 240 FPS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"一个ma35D AV1 codec 能处理1080p的数据流最大到多少fps \"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_tree_summarize.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   response.print_response_stream()\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_refine = index.as_query_engine(response_mode='refine', similarity_top_k=100)\n",
    "\n",
    "template = (\n",
    "    \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"answer the question in chinese\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9444ea0f73248cf8bfbdca7cb481eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:06:33 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\n",
      "Answer:To use the FFmpeg decoder to decode a clip that is already encoded in H.264 and save it to disk in a RAW format, you can use the following command:\n",
      "\n",
      "ffmpeg -i <INPUT> -c:v h264_ama -vf hwdownload,format=nv12 -out_fmt nv12 /tmp/dec_out.nv12\n",
      "\n",
      "This command will decode the input file using the H.264 AMA decoder and save it to disk in a RAW format with an NV12 pixel format.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa8a54aeae54630a6b13941d26fba34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:06:34 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question2: Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\n",
      "Answer:Here is the command:\n",
      "\n",
      "ffmpeg -hwaccel ama -i <INPUT> -vf \"hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y output.mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3476f17353e4bef884cb48e14296e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:06:36 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question3:  Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \n",
      "Answer:To convert YUV 4:2:2 10-bit pixel format to YUV 4:2:0 8-bit format using FFmpeg, you can use the following command:\n",
      "\n",
      "ffmpeg -i input.yuv420p10le -vf scale=flags=accurate_rnd,yuv2rgb,format=yuv422,setsar=1/2 output.yuv420p\n",
      "\n",
      "This command will convert your YUV 4:2:2 10-bit pixel format to YUV 4:2:0 8-bit format.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b0820764de4062973404456f931ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:06:39 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question4:  Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\n",
      "Answer:You can use the following command:\n",
      "\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v rawvideo -f rawvideo output_1080p.raw\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v rawvideo -f rawvideo output_720p.raw\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v rawvideo -f rawvideo output_480p.raw\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v rawvideo -f rawvideo output_240p.raw\n",
      "\n",
      "Replace \"input.mp4\" with the path to your existing H.264 file, and \"w\" and \"h\" with the desired width and height for each resolution (e.g., 1920x1080 for 10.80p).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb5d3302ba445cbbb89ef2c42793cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:06:43 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question5:  Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\n",
      "Answer:Here's a command that uses `ffmpeg` to decode an existing H.264 file, scale it into 1080p, 720p, 480p, and 240p, and save the raw outputs to disk:\n",
      "\n",
      "```\n",
      "ffmpeg -i <INPUT> -filter_complex \"[0]scale=10:9[1080];[0]scale=w:h[720];[0]scale=w:h[480];[0]scale=w:h[240]\" -vsync 0 -f rawvideo -s:v 10 -i /dev/zero <OUT DIR>/1080p.raw <OUT DIR>/720p.raw <OUT DIR>/480p.raw <OUT DIR>/240p.raw\n",
      "```\n",
      "\n",
      "This command uses the `scale` filter to scale the input video into four different resolutions: 1080p, 720p, 480p, and 240p. The `-vsync 0` option is used to disable vsync, which allows for raw output. The `-f rawvideo -s:v 10` options specify that the output should be in raw video format with a bit depth of 10 (i.e., 10-bit). The `i /dev/zero` option is used to create a zero-length file as the input for the raw video output.\n",
      "\n",
      "Note: Replace `<INPUT>` and `<OUT DIR>` with your actual input file name and output directory, respectively.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"\"\"Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\"\"\",\n",
    "   \"\"\"Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\"\"\",\n",
    "   \"\"\" Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \"\"\",\n",
    "   \"\"\" Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "   \"\"\" Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "   # \"\"\"ffmpeg命令使用ma35d硬件转码, 用一条命令行使用split方式，将一个h264 4k60的文件同时转码成两个hevc和av1格式的文件,写出具体的命令行例子\"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_engine = index.as_query_engine()\n",
    "   query_response = query_engine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c03ef679be744de9420e0f1a3b2e6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:06:46 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:50 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:53 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:06:56 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:00 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:03 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:06 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:08 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:11 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:13 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:17 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:21 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:25 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:28 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 15:07:32 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf \"hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4'\n",
      "Answer:Here's a rewritten answer in the context of the new configure options:\n",
      "\n",
      "This FFmpeg command efficiently encodes a raw video input into multiple-resolution outputs, leveraging AMD AMA Video SDK for processing. The command starts by specifying the input format as `rawvideo`, resolution as `19xx10xx`, and frame rate as `24`. The flags that follow apply to the selected stream.\n",
      "\n",
      "The command uses the `-hwaccel` option to utilize the AMD AMA Video SDK for hardware-accelerated video processing, followed by the `-f rawvideo` option to specify the input format. The resolution is set to `19xx10xx`, and the frame rate is set to `24`. The `-i cut1_10p.nv12` option specifies the input file.\n",
      "\n",
      "The command then applies a series of filters using the `-vf` option, starting with `hwupload` to upload the video data. This is followed by the encoding process, where the output format is specified as `av1_ama`, and the bitrate is set to `5M`. The final output file is saved to disk under `/path/to/1.av1_10p_1.mp4`.\n",
      "\n",
      "Note: I rewrote the answer using the new context provided.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf \"hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4'\"\"\",\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_refine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{response.response}\")\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8d1f4f63704b158fdea375296c962e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: how to enable ultra low latency for M35D Encoding?, give me a detailed ffmpge cmd example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 15:07:34 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, to enable ultra-low latency for x264 encoding, you can use the `-tune zerolatency` option.\n",
      "\n",
      "Here is an example FFmpeg command:\n",
      "\n",
      "```\n",
      "ffmpeg -i input -c:v libx264 -crf 26 -preset fast -tune zerolatency -c:a aac -b:a 128k output.mp4\n",
      "```\n",
      "\n",
      "This command uses the x264 encoder with the `zerolatency` tune option, which is designed for low-latency streaming. The `-crf` option sets the quality of the encoding, and the `-preset` option sets the encoding speed. The `-c:a aac -b:a 128k` options set the audio codec and bitrate.\n",
      "\n",
      "Note that this command assumes you want to encode video and audio simultaneously. If you only want to encode video or audio, you can modify the command accordingly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \n",
    "   \"how to enable ultra low latency for M35D Encoding?, give me a detailed ffmpge cmd example\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_tree_summarize.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   response.print_response_stream()\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
