{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # LLamaIndex 使用 PyTorch 进行向量计算\n",
    "# # 清理GPU 资源\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # 垃圾回收\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    " # --can not run in notebook, run it in cmdline\n",
    "# ! nvidia-smi --gpu-reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 数量： 2\n",
      "GPU 0: Tesla V100-PCIE-16GB，内存使用情况：12664.5625 MB / 16384.0 MB\n",
      "GPU 1: Tesla V100-PCIE-16GB，内存使用情况：4732.5625 MB / 16384.0 MB\n",
      "当前系统内存使用信息 pmem(rss=77864960, vms=720195584, shared=16674816, text=2818048, lib=0, data=148176896, dirty=0)\n",
      "物理内存中实际驻留的大小 0.07251739501953125 GB\n",
      "虚拟内存的大小，包括实际使用的物理内存和交换空间 0.6707344055175781 GB\n",
      "共享内存的大小，被多个进程共享的内存量 0.015529632568359375 GB\n",
      "可执行程序的代码段大小 0.00262451171875 GB\n",
      "程序的数据段大小 0.13800048828125 GB\n"
     ]
    }
   ],
   "source": [
    "# %pip install nvidia-ml-py3\n",
    "import pynvml\n",
    "# 初始化 NVML 库\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 获取 GPU 数量\n",
    "num_gpus = pynvml.nvmlDeviceGetCount()\n",
    "print(\"GPU 数量：\", num_gpus)\n",
    "\n",
    "# 遍历每个 GPU，获取其资源信息\n",
    "for i in range(num_gpus):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    gpu_memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU {i}: {gpu_name.decode()}，内存使用情况：{gpu_memory_info.used / 1024 / 1024} MB / {gpu_memory_info.total / 1024 / 1024} MB\")\n",
    "\n",
    "# 关闭 NVML 库\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "import psutil\n",
    "# 获取当前进程的内存信息\n",
    "process = psutil.Process()\n",
    "mem_info = process.memory_info()\n",
    "print(\"当前系统内存使用信息\",mem_info)\n",
    "print(\"物理内存中实际驻留的大小\", mem_info.rss/(1024*1024*1024), \"GB\")\n",
    "print(\"虚拟内存的大小，包括实际使用的物理内存和交换空间\", mem_info.vms/(1024*1024*1024), \"GB\")\n",
    "print(\"共享内存的大小，被多个进程共享的内存量\", mem_info.shared/(1024*1024*1024), \"GB\")\n",
    "print(\"可执行程序的代码段大小\", mem_info.text/(1024*1024*1024), \"GB\")\n",
    "print(\"程序的数据段大小\", mem_info.data/(1024*1024*1024), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "05/04/2024 12:08:36 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: maidalun1020/bce-embedding-base_v1\n",
      "/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "05/04/2024 12:08:41 - [INFO] -sentence_transformers.SentenceTransformer->>>    2 prompts are loaded, with the keys: ['query', 'text']\n",
      "05/04/2024 12:08:43 - [INFO] -BCEmbedding.models.RerankerModel->>>    Loading from `maidalun1020/bce-reranker-base_v1`.\n",
      "05/04/2024 12:08:44 - [INFO] -BCEmbedding.models.RerankerModel->>>    Execute device: cuda;\t gpu num: 2;\t use fp16: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %pip install llama-index-llms-ollama\n",
    "# !pip install llama-index\n",
    "# %pip install llama-index-embeddings-ollama\n",
    "# %pip install docx2txt\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-instructor\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# load the ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from BCEmbedding.tools.llama_index import BCERerank\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# model set\n",
    "llama2_7b = \"llama2\"\n",
    "llama2_13b = \"llama2:13b\"\n",
    "llama3_8b = \"llama3\"\n",
    "llama3_70b = \"llama3:70b\"\n",
    "\n",
    "\n",
    "# embedding_type = \"llama3\"\n",
    "embedding_type = \"bce\"\n",
    "rebuild = False\n",
    "\n",
    "if embedding_type == \"llama3\":\n",
    "    base_name = \"ma35_rag_base\"\n",
    "    embedding_model = OllamaEmbedding(model_name=llama3_8b,ollama_additional_kwargs={\"mirostat\": 0}) #base_url=\"http://localhost:11434\"\n",
    "elif embedding_type == \"bce\":\n",
    "    base_name = \"ma35_rag_base_bce\"\n",
    "    embed_args = {'model_name': 'maidalun1020/bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 256, 'device': 'cuda'}\n",
    "    embedding_model = HuggingFaceEmbedding(**embed_args)\n",
    "else:\n",
    "    print(\"embedding model not correct\\n\")\n",
    "    exit(-1)\n",
    "\n",
    "# connect with the ollama server\n",
    "llm_llama = Ollama(model=llama3_8b, request_timeout=600, temperature=0.1, device='cuda') #base_url = 'http://localhost:11434',\n",
    "# llm_llama2 = Ollama(model=\"llama2:13b\", request_timeout=600, temperature=0.1) #base_url = 'http://localhost:11434',\n",
    "\n",
    "reranker_args = {'model': 'maidalun1020/bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda'}\n",
    "reranker_model = BCERerank(**reranker_args)\n",
    "\n",
    "Settings.llm = llm_llama\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "# Settings.num_output = 512\n",
    "# Settings.context_window = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:08:44 - [INFO] -chromadb.telemetry.product.posthog->>>    Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# create a vector storage\n",
    "# %pip install llama-index-vector-stores-chroma\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "chroma_client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if rebuild :\n",
    "    # %pip install llama-index-readers-web\n",
    "\n",
    "    from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "    documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "        [\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/index.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/getting_started_on_prem.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/virtualization.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/tutorials.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/quality_analysis.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/filters.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/tutorials.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/filters.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xcompositor.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xabrladder.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/examples/xma/xma_apps.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/specs_and_features.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/package_feed.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/using_ffmpeg.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/using_gstreamer.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/unified_logging.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/tuning_video_quality.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/tuning_pipeline_latency.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/managing_compute_resources.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/c_apis.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/card_management.html\",\n",
    "        \"https://amd.github.io/ama-sdk/v1.1.2/encoder_comp_matrix.html\",\n",
    "        \"https://ffmpeg.org/ffmpeg.html\",\n",
    "        \"https://ffmpeg.org/ffmpeg-resampler.html\",\n",
    "        \"https://ffmpeg.org/ffmpeg-devices.html\",\n",
    "        \"https://ffmpeg.org/ffmpeg-all.html\",\n",
    "        \"https://trac.ffmpeg.org/wiki/Encode/H.264\",\n",
    "        \"https://trac.ffmpeg.org/wiki/Encode/H.265\",\n",
    "        \"https://trac.ffmpeg.org/wiki/Encode/AV1\",\n",
    "        \"https://trac.ffmpeg.org/wiki/Scaling\",\n",
    "        \"https://trac.ffmpeg.org/wiki/Null\",\n",
    "        \"https://trac.ffmpeg.org/wiki/FilteringGuide\",\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    collection_name = base_name\n",
    "    collection = chroma_client.list_collections()\n",
    "    if collection_name in collection:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "        chroma_client.clear_system_cache()\n",
    "    chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(docstore=documents, vector_store=vector_store)\n",
    "\n",
    "    # 这个nodes 有什么用处\n",
    "    from llama_index.core.node_parser import SimpleNodeParser \n",
    "    # Initialize the parser \n",
    "    parser = SimpleNodeParser.from_defaults(chunk_size=500, chunk_overlap=20) \n",
    "    # Parse documents into nodes \n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "    # print(nodes[0])\n",
    "    len(nodes)\n",
    "\n",
    "    # %pip install ipywidgets\n",
    "    # index = VectorStoreIndex.from_documents(documents,storage_context=storage_context,show_progress=True)\n",
    "    index = VectorStoreIndex(nodes,embed_model=embedding_model,storage_context=storage_context,show_progress=True)\n",
    "\n",
    "    # # documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # 读取 HTML 文件\n",
    "# with open(\"local_html/FFMPEG command line arguments - VideoDC - Xilinx Enterprise Wiki.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     html_content = file.read()\n",
    "\n",
    "# # 使用 Beautiful Soup 解析 HTML\n",
    "# soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# # 提取文档内容\n",
    "# document_text = soup.get_text()\n",
    "\n",
    "# # 打印文档内容\n",
    "# print(document_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index from stored vectors\n",
    "collection_name = base_name\n",
    "collection = chroma_client.list_collections()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, embed_model=embedding_model,storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \"\"\",\n",
    "\n",
    "   \"\"\"explain following ffmpeg command\n",
    "   ffmpeg -y -hwaccel ama \\\n",
    "      -c:v h264_ama  -out_fmt nv12 -i <INPUT>  \\\n",
    "      -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; \\\n",
    "                     [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" \\\n",
    "      -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv \\\n",
    "      -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv  \\\n",
    "      -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv \\\n",
    "      -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\"\"\",\n",
    "\n",
    "    \n",
    "    \"\"\"Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\"\"\",\n",
    "    \"\"\"Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\"\"\",\n",
    "    \"\"\" Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \"\"\",\n",
    "    \"\"\" Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "    \"\"\" Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "\n",
    "    \"\"\"How to tuning Latency of MA35D ama codec video transcode,  for example enable ultra low latency for ama_av1 Encoding?\"\"\",\n",
    "    \n",
    "    \"一个ma35D AV1 codec transcode 的处理能力是多少 \",\n",
    "\n",
    "   # the question come from forum\n",
    "    \"\"\"Is max_bitrate implemented in ama sdk ffmpeg, and is it correct that the max bitrate parameter in AMA is irrelevant to VBR?\"\"\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = index.as_retriever()\n",
    "# relevant_docs = retriever.retrieve(\"what is the max transcode rate for 1080p30 stream\")\n",
    "# relevant_docs\n",
    "\n",
    "# \"\"\"\n",
    "# response_mode\n",
    "\n",
    "#     REFINE = \"refine\"\n",
    "#     COMPACT = \"compact\"\n",
    "#     SIMPLE_SUMMARIZE = \"simple_summarize\"\n",
    "#     TREE_SUMMARIZE = \"tree_summarize\"\n",
    "#     GENERATION = \"generation\"\n",
    "#     NO_TEXT = \"no_text\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# ResponseMode为tree_summarize时，LLM会对每一段文本进行最大长度的分割，并进行连续的读取和询问。这种模式的优点是可以保证对文本的完整理解和回答，但如果没有正确处理分割段落的情况，可能会导致错误的生成结果\n",
    "# ResponseMode为generation时，生成的回答不依赖于文档的内容，只基于提供的问题进行生成。这种模式适用于纯粹的问题回\n",
    "# ResponseMode为no_text时，生成的回答中不包含任何内容，仅作为占位符使用\n",
    "# ResponseMode为simple_summarize时，LLM会截取每段文本的相关句子（通常是第一句），并进行提炼生成回答。这种模式适用于对结果要求不高的场景。\n",
    "# ResponseMode为refine时，如果只有一个文本块（text_chunk），则会正常生成回答。但如果存在多个文本块，则会以类似轮询的方式迭代生成回答。这种模式可以对多个文本块进行迭代式的回答生成，逐步完善回答内容。\n",
    "# ResponseMode为compact时，生成的回答会将多个文本块（text_chunk）压缩到设定的最大长度，并生成一次回答。然后，根据后续内容对以往的答案进行改进和完善（即进行多次迭代）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are hellpful, respectful and honest video transcode assistant and very faimilay with ffmpge, and expecially good at MA35D AMA(AMD multimidia accelerator) device encode/decode/transcode.\n",
      "Context information from multiple sources is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the information from multiple sources and not prior knowledge\n",
      "please read the above context information carefully. and anwer the question.\n",
      "if the question is not releate with video process, just say it is not releated with my knowledge base.\n",
      "if you don't know the answer, just say that I don't know.\n",
      "Answers need to be precise and concise.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine = index.as_query_engine(response_mode='simple_summarize', similary_threshold=0.1, similarity_top_k=5)\n",
    "\n",
    "template = (\n",
    "    \"You are hellpful, respectful and honest video transcode assistant and very faimilay with ffmpge, and expecially good at MA35D AMA(AMD multimidia accelerator) device encode/decode/transcode.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b9e0d30d6c433289541f8f95bdc974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:08:54 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \n",
      "Answer:This FFmpeg command is used to encode a raw video file with the YUV 4:2:0 (NV12) format into an MP4 container using the AV1 codec.\n",
      "\n",
      "Here's a breakdown of the command:\n",
      "\n",
      "* `-hwaccel ama`: This flag enables the AMD MultiMedia Accelerator (AMA) hardware acceleration for the FFmpeg process.\n",
      "* `-f rawvideo`: This flag specifies that the input file is in raw video format.\n",
      "* `-s 19:20x10:80`: This flag sets the resolution of the input file to 1920x1080 pixels.\n",
      "* `-framerate 24`: This flag sets the frame rate of the input file to 24 frames per second.\n",
      "* `-i cut1_10:80p.nv12`: This flag specifies the input file, which is a raw video file with the YUV 4:2:0 (NV12) format and a resolution of 1920x1080 pixels.\n",
      "* `-vf 'hwupload'`: This flag applies the `hwupload` filter to the input file. The `hwupload` filter uploads the input data to the AMA hardware accelerator for processing.\n",
      "* `-c:v av1_ama`: This flag specifies that the video codec is AV1, which is accelerated by the AMA hardware accelerator.\n",
      "* `-b:v 5M`: This flag sets the target bitrate of the encoded video stream to 5 megabits per second.\n",
      "* `-f mp4`: This flag specifies that the output file should be in MP4 format.\n",
      "* `-y 1.av1_10:80p_1.mp4`: This flag specifies the output file name, which is `1.av1_1080p_1.mp4`.\n",
      "\n",
      "In summary, this FFmpeg command encodes a raw video file with the YUV 4:2:0 (NV12) format into an MP4 container using the AV1 codec and AMA hardware acceleration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7932135a552344f5b268ba2a54111070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:05 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question2: explain following ffmpeg command\n",
      "   ffmpeg -y -hwaccel ama       -c:v h264_ama  -out_fmt nv12 -i <INPUT>        -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d];                      [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\"       -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv       -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv        -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv       -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
      "Answer:This FFmpeg command is used to scale a video input to four different resolutions (1920x1080, 1280x720, 720x480, and 360x240) using the `scaler_ama` filter, which is a hardware-accelerated scaler. The output of each resolution is then saved as a raw YUV file.\n",
      "\n",
      "Here's a breakdown of the command:\n",
      "\n",
      "* `-y`: Overwrite output files without asking.\n",
      "* `-hwaccel ama`: Use the AMA (AMD MultiMedia Accelerator) hardware accelerator for video processing.\n",
      "* `-c:v h264_ama`: Set the video codec to H.264, using the AMA hardware accelerator.\n",
      "* `-out_fmt nv12`: Set the output format to NV12 (a planar YUV format).\n",
      "* `-i <INPUT>`: Input the video file specified by `<INPUT>`.\n",
      "* `filter_complex`: Apply a complex filter graph to the input video.\n",
      "\n",
      "The filter graph consists of two parts:\n",
      "\n",
      "1. The first part scales the input video to four different resolutions using the `scaler_ama` filter:\n",
      "\t* `scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d];`\n",
      "\t\t+ This sets up four output streams, each with a different resolution.\n",
      "\t\t+ The `[a]`, `[b]`, `[c]`, and `[d]` labels are used to identify the outputs.\n",
      "2. The second part downloads each of the scaled video streams and formats them as NV12:\n",
      "\t* `hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1];`\n",
      "\t\t+ This downloads each of the scaled video streams and formats them as NV12.\n",
      "\n",
      "The command then maps each of the formatted video streams to a raw YUV file using the `-map` option:\n",
      "\n",
      "* `map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv`\n",
      "\t+ This maps the first output stream (`[a1]`) to a raw YUV file named `/tmp/scale_1080p.yuv`.\n",
      "* Similarly, the other three output streams are mapped to files named `/tmp/scale_720p.yuv`, `/tmp/scale_480p.yuv`, and `/tmp/scale_240p.yuv`.\n",
      "\n",
      "In summary, this command scales a video input to four different resolutions using the `scaler_ama` filter, formats each of the scaled streams as NV12, and saves each stream as a raw YUV file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4340d0ecbd0421a9cce57960de728a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:08 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question3: Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\n",
      "Answer:To use FFmpeg decoder to decode a clip that is already encoded in H.264 and save it to disk in RAW format, you can use the following command:\n",
      "\n",
      "```\n",
      "ffmpeg -y -hwaccel ama -c:v h264_ama -out_fmt nv12 -i <INPUT> -vf hwdownload,format=nv12 -f rawvideo /tmp/dec_out.nv12\n",
      "```\n",
      "\n",
      "This command uses the `h264_ama` decoder to decode the H.264 encoded clip and save it in RAW format (`nv12`) to disk. The `-vf hwdownload,format=nv12` filter is used to convert the decoded video into a host-readable buffer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8939c44b26aa40a8820e2787c8914ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:11 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question4: Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\n",
      "Answer:Here is the command:\n",
      "\n",
      "```\n",
      "ffmpeg -hwaccel ama -i <INPUT> -vf \"format=yuv420p, hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y output.mp4\n",
      "```\n",
      "\n",
      "Note that I did not use the `-re` option as per your request.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a1ba14fd0d4acc88654fe3223183b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:16 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question5:  Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \n",
      "Answer:To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format using ffmpeg, you can use the following command:\n",
      "\n",
      "```\n",
      "ffmpeg -hwaccel ama -i <INPUT> -vf \"format=yuv420p,hwupload\" -c:v h264_ama -b:v 1M <OUTPUT>\n",
      "```\n",
      "\n",
      "This command uses the hardware-accelerated AV1 encoder (`-c:v av1_ama`) to convert the input YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format. The `format=yuv420p,hwupload` filter is used to upload and convert the input video as YUV 4:2:0 8 bit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529525c6ce7449b99ca430a9461c0c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:23 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question6:  Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\n",
      "Answer:Based on the provided context information, I can help you with that.\n",
      "\n",
      "To decode an existing H.264 file and scale it into different resolutions (10.8p, 720p, 480p, and 240p), you can use the following command:\n",
      "\n",
      "```\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v libx264 -b:v 1M output_1080p.mp4\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v libx264 -b:v 1M -s 1280x720 output_720p.mp4\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v libx264 -b:v 1M -s 640x480 output_480p.mp4\n",
      "ffmpeg -i input.mp4 -vf scale=w:h -c:v libx264 -b:v 1M -s 320x240 output_240p.mp4\n",
      "```\n",
      "\n",
      "In this command:\n",
      "\n",
      "* `-i input.mp4` specifies the input file.\n",
      "* `-vf scale=w:h` scales the video to a new width (`w`) and height (`h`). You can replace `w` and `h` with the desired values for each resolution (e.g., 1920x1080, 1280x720, etc.).\n",
      "* `-c:v libx264` specifies the output codec as H.264.\n",
      "* `-b:v 1M` sets the target bitrate to 1 megabit per second.\n",
      "* `output_1080p.mp4`, `output_720p.mp4`, `output_480p.mp4`, and `output_240p.mp4` specify the output file names for each resolution.\n",
      "\n",
      "Please note that you need to replace `input.mp4` with your actual input file name, and adjust the scaling values (`w` and `h`) according to your desired resolutions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317b5355d8264c36a8f9492432617315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:30 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question7:  Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\n",
      "Answer:Based on the provided context information, here is a possible command line that achieves the desired result:\n",
      "\n",
      "```\n",
      "ffmpeg -y -hwaccel ama -c:v h264_ama -out_fmt nv12 -i <INPUT> -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
      "```\n",
      "\n",
      "This command line uses the `scaler_ama` filter to scale the input H.264 file into four different resolutions (1920x1080, 1280x720, 720x480, and 360x240) and saves the RAW outputs to disk under `/tmp/`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdeaa2e155504872b249548e9c3c897c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:33 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question8: How to tuning Latency of MA35D ama codec video transcode,  for example enable ultra low latency for ama_av1 Encoding?\n",
      "Answer:According to the provided context information, to tune the latency of MA35D AMA codec video transcode, you can use the `-lookahead_depth` option. For enabling ultra-low latency for `ama_av1` encoding, you can set the `tune` mode to \"Ultra Low Latency\" and adjust the `latency` setting accordingly.\n",
      "\n",
      "Here's an example command:\n",
      "```\n",
      "ffmpeg -y -hwaccel ama -c:v ama_av1 -i <INPUT> -tune ultra_low_latency -lookahead_depth 0 [other options]\n",
      "```\n",
      "Note that you may need to adjust other parameters, such as the `preset`, to achieve the desired balance between latency and quality.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476dae02b1174a588c680d37632d2e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:37 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question9: 一个ma35D AV1 codec transcode 的处理能力是多少 \n",
      "Answer:Based on the provided context information, it seems that the MA35D card is capable of transcoding to and from HEVC, AVC, and AV1 formats at an aggregate rate of up to two 4Kp60 for AVC and HEVC formats, and four 4Kp60 for AV1 format, per VPU.\n",
      "\n",
      "However, I couldn't find any specific information on the processing power or throughput of the MA35D card for a single AV1 codec transcode. The provided tables seem to focus more on the video processing power of the MA35D cards in general, rather than a specific transcode scenario.\n",
      "\n",
      "Therefore, I don't have enough information to provide an accurate answer to this question.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5a5ece063d4838aa2ab559663a2907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2024 12:09:42 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question10: Is max_bitrate implemented in ama sdk ffmpeg, and is it correct that the max bitrate parameter in AMA is irrelevant to VBR?\n",
      "Answer:Based on the provided context information, I can answer your question as follows:\n",
      "\n",
      "The `max_bitrate` parameter is not explicitly mentioned in the AMA SDK documentation. However, according to the FFmpeg documentation, the `-maxrate` and `-bufsize` options are used to constrain the maximum bitrate used by the encoder.\n",
      "\n",
      "Regarding the relevance of the `max_bitrate` parameter to VBR (Variable Bit Rate), it seems that the `max_bitrate` is indeed relevant to VBR. According to the FFmpeg documentation, when using VBR, the encoder will adjust the bitrate per-frame to achieve a certain average bitrate. The `-maxrate` and `-bufsize` options are used to constrain this average bitrate.\n",
      "\n",
      "In summary:\n",
      "\n",
      "* The `max_bitrate` parameter is not explicitly mentioned in the AMA SDK documentation.\n",
      "* However, according to FFmpeg documentation, the `-maxrate` and `-bufsize` options are used to constrain the maximum bitrate used by the encoder, which is relevant to VBR.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here no database input, so no answer\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "   \n",
    "\n",
    "   # print(\" \")\n",
    "   # print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   # for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "   #    print(result)\n",
    "\n",
    "   #    # print(f\"Result {i}: Document ID {result['id']}, Title '{result['title']}', Similarity: {result.score}\")\n",
    "   #    print(f\"Result {i}\\n Similarity: {result.score}\\n content '{result.get_content()}\")\n",
    "\n",
    "   # print()\n",
    "   # # print(response.get_formatted_sources(length=10))\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are hellpful, respectful and honest video transcode assistant and very faimilay with ffmpge, and expecially good at MA35D AMA(AMD multimidia accelerator) device encode/decode/transcode.\n",
      "Context information from multiple sources is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the information from multiple sources and not prior knowledge\n",
      "please read the above context information carefully. and anwer the question.\n",
      "if the question is not releate with video process, just say it is not releated with my knowledge base.\n",
      "if you don't know the answer, just say that I don't know.\n",
      "Answers need to be precise and concise.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}.\n",
      "We have provided an existing answer: {existing_answer}.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "-------------\n",
      "{context_msg}\n",
      "-------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "if the question is 'who are you' , just say I am a video expert.\n",
      "Answers need to be precise and concise.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_rerank = index.as_query_engine(response_mode='refine',similarity_top_k=50, temperature=0.6,node_postprocessors=[reranker_model])\n",
    "\n",
    "template = (\n",
    "    \"You are hellpful, respectful and honest video transcode assistant and very faimilay with ffmpge, and expecially good at MA35D AMA(AMD multimidia accelerator) device encode/decode/transcode.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine_rerank.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine_rerank.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")\n",
    "\n",
    "prompts_dict = query_engine_rerank.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160ac20d2add47de9e0252d0587fb668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "05/04/2024 12:09:50 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "05/04/2024 12:09:57 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "05/04/2024 12:10:04 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "05/04/2024 12:10:12 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "05/04/2024 12:10:20 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \n",
      "Answer:Based on the provided context information, I can refine the given FFmpeg command:\n",
      "\n",
      "The command is used to transcode a RAW video input file `cut1_10:80p.nv12` into an MP4 container with AV1 codec using AMD MultiMedia Accelerator (AMA) hardware acceleration.\n",
      "\n",
      "Here's a breakdown of the flags:\n",
      "\n",
      "* `-hwaccel ama`: This flag enables the AMD MultiMedia Accelerator (AMA) hardware acceleration for the FFmpeg pipeline.\n",
      "* `-f rawvideo`: The input file format is set to RAW video, which means that the input file does not contain any container or metadata information.\n",
      "* `-s 19:20x10:80`: The resolution of the input file is specified as 19:20x10:80 (10:80p).\n",
      "* `-framerate 24`: The target frame rate for the input file is set to 24 frames per second.\n",
      "* `-i cut1_10:80p.nv12`: This flag specifies the input file name, which is `cut1_10:80p.nv12`.\n",
      "* `-vf 'hwupload'`: This flag enables the hardware upload of the video data using the AMA accelerator. The output format is set to YUV420P.\n",
      "* `-c:v av1_ama`: The video codec for the output file is set to AV1, which is a hardware-accelerated encoder using the AMA accelerator.\n",
      "* `-b:v 5M`: The target bitrate for the encoded stream is set to 5 megabits per second (Mbps).\n",
      "* `-f mp4 -y 1.av1_10:80p_1.mp4`: The output file format is set to MP4, and the output file name is specified as `1.av1_10:80p_1.mp4`.\n",
      "\n",
      "In summary, this FFmpeg command transcodes a RAW video input file into an MP4 container with AV1 codec using AMD MultiMedia Accelerator (AMA) hardware acceleration.\n",
      "\n",
      "Additional context provided shows that the same FFmpeg command can be used to encode only into multiple-resolution outputs. This is achieved by specifying different output resolutions and frame rates in the `filter_complex` option, and then mapping each of these outputs to a separate MP4 file using the `-map` option.\n",
      "\n",
      "Refined Answer:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d78d66688864218bcf46c417204838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 1208, in forward\n    outputs = self.roberta(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 837, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 525, in forward\n    layer_outputs = layer_module(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 414, in forward\n    self_attention_outputs = self.attention(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 341, in forward\n    self_outputs = self.self(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 237, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 375.44 MiB is free. Process 2068 has 4.25 GiB memory in use. Process 267664 has 7.89 GiB memory in use. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.55 GiB is allocated by PyTorch, and 264.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[1;32m      6\u001b[0m    counter \u001b[38;5;241m=\u001b[39m counter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m    query_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine_rerank\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m dispatch_event(QueryEndEvent())\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:189\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    187\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    188\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 189\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[1;32m    191\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[1;32m    192\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:145\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m    144\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retriever\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_node_postprocessors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:138\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._apply_node_postprocessors\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_node_postprocessors\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m, nodes: List[NodeWithScore], query_bundle: QueryBundle\n\u001b[1;32m    136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_postprocessor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_postprocessors:\n\u001b[0;32m--> 138\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[43mnode_postprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/llama_index/core/postprocessor/types.py:55\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[0;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/BCEmbedding/BCEmbedding/tools/llama_index/bce_rerank.py:76\u001b[0m, in \u001b[0;36mBCERerank._postprocess_nodes\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m     64\u001b[0m         invalid_nodes\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     67\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRERANKING,\n\u001b[1;32m     68\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         },\n\u001b[1;32m     74\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m---> 76\u001b[0m     rerank_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     new_nodes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m score, nid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(rerank_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrerank_scores\u001b[39m\u001b[38;5;124m'\u001b[39m], rerank_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrerank_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/BCEmbedding/BCEmbedding/models/reranker.py:136\u001b[0m, in \u001b[0;36mRerankerModel.rerank\u001b[0;34m(self, query, passages, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    129\u001b[0m         sentence_pairs[k:k\u001b[38;5;241m+\u001b[39mbatch_size],\n\u001b[1;32m    130\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m batch_on_device \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 136\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_on_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    137\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(scores)\n\u001b[1;32m    138\u001b[0m tot_scores\u001b[38;5;241m.\u001b[39mextend(scores\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 1208, in forward\n    outputs = self.roberta(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 837, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 525, in forward\n    layer_outputs = layer_module(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 414, in forward\n    self_attention_outputs = self.attention(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 341, in forward\n    self_outputs = self.self(\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 237, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 375.44 MiB is free. Process 2068 has 4.25 GiB memory in use. Process 267664 has 7.89 GiB memory in use. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.55 GiB is allocated by PyTorch, and 264.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# questions = [\n",
    "# ]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine_rerank.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "\n",
    "   print(\"\")\n",
    "   # print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   # for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "   #    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# response_mode 有几种模式可选，refine， compact, tree_summarize 等，每一种都有对应的promopt template\n",
    "query_engine_tree_summarize = index.as_query_engine(response_mode='tree_summarize',similary_threshold=0.1, similarity_top_k=30, node_postprocessors=[reranker_model])\n",
    "\n",
    "template = (\n",
    "    \"You are a Video ffmpeg & gstreamer technolodge expert and expecially good at MA35D AMA(AMD multimidia accelerator) device encode/decode/scale/transcode.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge, please read the sources carefully.\\n\"\n",
    "    \"if the question is not releate with the RDMA, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a FPGA and RDMA expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine_tree_summarize.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": qa_template}\n",
    ")\n",
    "\n",
    "prompts_dict = query_engine_tree_summarize.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = [\n",
    "#    \"一个ma35D AV1 codec 能处理1080p的数据流最大到多少fps \",\n",
    "#     \"\"\"for MA35D video transcode, how to tuning Latency,  for example enable ultra low latency for ama_av1 Encoding?\"\"\",\n",
    "#    \"\"\"does AMA SDK FFmpeg implement `max_bitrate`, Is max_bitrate implemented in ma35d sdk ffmpeg, and is it correct that the max bitrate parameter in AMA is irrelevant to VBR?\"\"\",\n",
    "# ]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine_tree_summarize.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "\n",
    "   print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_refine = index.as_query_engine(response_mode='refine', similarity_top_k=20)\n",
    "\n",
    "template = (\n",
    "    \"You are video transcode expert and very faimilay with ffmpge and expecially good at MA35D AMA(AMD multimidia accelerator) device encode/decode/scale/transcode.\\.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"if the question is in chinese, please transclate chinese to english in advance\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")\n",
    "\n",
    "prompts_dict = query_engine_refine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = [\n",
    "#    \"\"\"Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\"\"\",\n",
    "#    \"\"\"Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\"\"\",\n",
    "#    \"\"\" Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \"\"\",\n",
    "#    \"\"\" Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "#    \"\"\" Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "#    # \"\"\"ffmpeg命令使用ma35d硬件转码, 用一条命令行使用split方式，将一个h264 4k60的文件同时转码成两个hevc和av1格式的文件,写出具体的命令行例子\"\"\"\n",
    "# ]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   # query_engine = index.as_query_engine()\n",
    "   query_response = query_engine_refine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = [\n",
    "\n",
    "# ]\n",
    "\n",
    "# counter = 0\n",
    "# for question in questions:\n",
    "#    counter = counter + 1\n",
    "#    query_engine = index.as_query_engine()\n",
    "#    query_response = query_engine.query(question)\n",
    "#    print(f\"Question{counter}: {question}\")\n",
    "#    print(f\"Answer:{query_response.response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
