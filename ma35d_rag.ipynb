{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nvidia-ml-py3\n",
    "import pynvml\n",
    "\n",
    "# 初始化 NVML 库\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 获取 GPU 数量\n",
    "num_gpus = pynvml.nvmlDeviceGetCount()\n",
    "print(\"GPU 数量：\", num_gpus)\n",
    "\n",
    "# 遍历每个 GPU，获取其资源信息\n",
    "for i in range(num_gpus):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    gpu_memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU {i}: {gpu_name.decode()}，内存使用情况：{gpu_memory_info.used / 1024 / 1024} MB / {gpu_memory_info.total / 1024 / 1024} MB\")\n",
    "\n",
    "# 关闭 NVML 库\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# LLamaIndex 使用 PyTorch 进行向量计算\n",
    "# 清理GPU 资源\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 NVML 库\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 获取 GPU 数量\n",
    "num_gpus = pynvml.nvmlDeviceGetCount()\n",
    "print(\"GPU 数量：\", num_gpus)\n",
    "\n",
    "# 遍历每个 GPU，获取其资源信息\n",
    "for i in range(num_gpus):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    gpu_memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU {i}: {gpu_name.decode()}，内存使用情况：{gpu_memory_info.used / 1024 / 1024} MB / {gpu_memory_info.total / 1024 / 1024} MB\")\n",
    "\n",
    "# 关闭 NVML 库\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %pip install llama-index-llms-ollama\n",
    "# !pip install llama-index\n",
    "# %pip install llama-index-embeddings-ollama\n",
    "# %pip install docx2txt\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-instructor\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# load the ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from BCEmbedding.tools.llama_index import BCERerank\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# model set\n",
    "llama2_7b = \"llama2\"\n",
    "llama2_13b = \"llama2:13b\"\n",
    "llama3_8b = \"llama3\"\n",
    "llama3_70b = \"llama3:70b\"\n",
    "\n",
    "# connect with the ollama server, and talk with llama2 \n",
    "\n",
    "llm_llama = Ollama(model=llama3_8b, request_timeout=600, temperature=0.1, device='cuda') #base_url = 'http://localhost:11434',\n",
    "# llm_llama2 = Ollama(model=\"llama2:13b\", request_timeout=600, temperature=0.1) #base_url = 'http://localhost:11434',\n",
    "# embedding_model = OllamaEmbedding(model_name=\"llama2\",ollama_additional_kwargs={\"mirostat\": 0}) #base_url=\"http://localhost:11434\"\n",
    "\n",
    "embed_args = {'model_name': 'maidalun1020/bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 256, 'device': 'cuda'}\n",
    "embedding_model = HuggingFaceEmbedding(**embed_args)\n",
    "\n",
    "reranker_args = {'model': 'maidalun1020/bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda'}\n",
    "reranker_model = BCERerank(**reranker_args)\n",
    "\n",
    "Settings.llm = llm_llama\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "# Settings.num_output = 512\n",
    "# Settings.context_window = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector storage\n",
    "# %pip install llama-index-vector-stores-chroma\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "chroma_client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install llama-index-readers-web\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/index.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/getting_started_on_prem.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/virtualization.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/tutorials.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/quality_analysis.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/filters.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/tutorials.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/filters.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xcompositor.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xabrladder.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/xma/xma_apps.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/specs_and_features.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/package_feed.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/using_ffmpeg.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/using_gstreamer.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/unified_logging.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/tuning_video_quality.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/tuning_pipeline_latency.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/managing_compute_resources.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/c_apis.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/card_management.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/encoder_comp_matrix.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-resampler.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-devices.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-all.html\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/H.264\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/H.265\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/AV1\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Scaling\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Null\",\n",
    "     \"https://trac.ffmpeg.org/wiki/FilteringGuide\",\n",
    "     ]\n",
    "     \n",
    ")\n",
    "\n",
    "collection_name = \"ma35_rag_base_beg\"\n",
    "collection = chroma_client.list_collections()\n",
    "if collection_name in collection:\n",
    "    chroma_client.delete_collection(collection_name)\n",
    "    chroma_client.clear_system_cache()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(docstore=documents, vector_store=vector_store)\n",
    "\n",
    "# 这个nodes 有什么用处\n",
    "from llama_index.core.node_parser import SimpleNodeParser \n",
    "# Initialize the parser \n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=500, chunk_overlap=20) \n",
    "# Parse documents into nodes \n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "# print(nodes[0])\n",
    "len(nodes)\n",
    "\n",
    "# %pip install ipywidgets\n",
    "# index = VectorStoreIndex.from_documents(documents,storage_context=storage_context,show_progress=True)\n",
    "index = VectorStoreIndex(nodes,embed_model=embedding_model,storage_context=storage_context,show_progress=True)\n",
    "\n",
    "# # documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # 读取 HTML 文件\n",
    "# with open(\"local_html/FFMPEG command line arguments - VideoDC - Xilinx Enterprise Wiki.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     html_content = file.read()\n",
    "\n",
    "# # 使用 Beautiful Soup 解析 HTML\n",
    "# soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# # 提取文档内容\n",
    "# document_text = soup.get_text()\n",
    "\n",
    "# # 打印文档内容\n",
    "# print(document_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your index from stored vectors\n",
    "\n",
    "collection_name = \"ma35_rag_base\"\n",
    "collection = chroma_client.list_collections()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, embed_model=embedding_model,storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = index.as_retriever()\n",
    "# relevant_docs = retriever.retrieve(\"what is the max transcode rate for 1080p30 stream\")\n",
    "# relevant_docs\n",
    "\n",
    "# \"\"\"\n",
    "# response_mode\n",
    "\n",
    "#     REFINE = \"refine\"\n",
    "#     COMPACT = \"compact\"\n",
    "#     SIMPLE_SUMMARIZE = \"simple_summarize\"\n",
    "#     TREE_SUMMARIZE = \"tree_summarize\"\n",
    "#     GENERATION = \"generation\"\n",
    "#     NO_TEXT = \"no_text\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# ResponseMode为tree_summarize时，LLM会对每一段文本进行最大长度的分割，并进行连续的读取和询问。这种模式的优点是可以保证对文本的完整理解和回答，但如果没有正确处理分割段落的情况，可能会导致错误的生成结果\n",
    "# ResponseMode为generation时，生成的回答不依赖于文档的内容，只基于提供的问题进行生成。这种模式适用于纯粹的问题回\n",
    "# ResponseMode为no_text时，生成的回答中不包含任何内容，仅作为占位符使用\n",
    "# ResponseMode为simple_summarize时，LLM会截取每段文本的相关句子（通常是第一句），并进行提炼生成回答。这种模式适用于对结果要求不高的场景。\n",
    "# ResponseMode为refine时，如果只有一个文本块（text_chunk），则会正常生成回答。但如果存在多个文本块，则会以类似轮询的方式迭代生成回答。这种模式可以对多个文本块进行迭代式的回答生成，逐步完善回答内容。\n",
    "# ResponseMode为compact时，生成的回答会将多个文本块（text_chunk）压缩到设定的最大长度，并生成一次回答。然后，根据后续内容对以往的答案进行改进和完善（即进行多次迭代）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine = index.as_query_engine(response_mode='simple_summarize')\n",
    "\n",
    "# template = (\n",
    "#     \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "#     \"Context information from multiple sources is below.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "#     \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "#     \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "#     \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "#     \"Answers need to be precise and concise.\\n\"\n",
    "#     \"Query: {query_str}\\n\"\n",
    "#     \"Answer: \"\n",
    "# )\n",
    "# qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "# query_engine.update_prompts(\n",
    "#     {\"response_synthesizer:text_qa_template\": qa_template}\n",
    "# )\n",
    "\n",
    "\n",
    "# template = (\n",
    "#     \"The original query is as follows: {query_str}.\\n\"\n",
    "#     \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "#     \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "#     \"-------------\\n\"\n",
    "#     \"{context_msg}\\n\"\n",
    "#     \"-------------\\n\"\n",
    "#     \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "#     \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "#     \"Answers need to be precise and concise.\\n\"\n",
    "#     \"Refined Answer: \"\n",
    "# )\n",
    "\n",
    "\n",
    "# qa_template = PromptTemplate(template)\n",
    "\n",
    "# query_engine.update_prompts(\n",
    "#     {\"response_synthesizer:refine_template\": qa_template}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here no database input, so no answer\n",
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command\\n\n",
    "   \n",
    "   \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \"\"\",\n",
    "\n",
    "   # \"\"\"\n",
    "   # explain following ffmpeg command\\n\n",
    "\n",
    "   # ffmpeg -y -hwaccel ama \\\n",
    "   #    -c:v h264_ama  -out_fmt nv12 -i <INPUT>  \\\n",
    "   #    -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; \\\n",
    "   #                   [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" \\\n",
    "   #    -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv \\\n",
    "   #    -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv  \\\n",
    "   #    -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv \\\n",
    "   #    -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
    "   # \"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "   print(\" \")\n",
    "\n",
    "   \n",
    "   print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "      print(result)\n",
    "\n",
    "      # print(f\"Result {i}: Document ID {result['id']}, Title '{result['title']}', Similarity: {result.score}\")\n",
    "      print(f\"Result {i}\\n Similarity: {result.score}\\n content '{result.get_content()}\")\n",
    "\n",
    "   # print()\n",
    "   # # print(response.get_formatted_sources(length=10))\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"ma35_rag_base_beg\"\n",
    "collection = chroma_client.list_collections()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, embed_model=embedding_model,storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_beg = index.as_query_engine(response_mode='refine',similarity_top_k=50, temperature=0.6,node_postprocessors=[reranker_model])\n",
    "\n",
    "template = (\n",
    "    \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"you must answer every question in chinese\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine_beg.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine_beg.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command 'ffmpeg -hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama -i infile.mp4 -c:v hevc_ama -b:v 1000K -r 60 -f mp4 -y transcoded.mp4' \"\"\",\n",
    "   \"\"\"详细解释一下下面的命令行 'gst-launch-1.0 filesrc location=<INPUT> ! parsebin ! h264parse ! ama_h264dec ! capsfilter 'caps=video/x-raw(memory:AMAMemory),format=NV12' !  ama_download ! filesink location=/tmp/h264.nv12'\"\"\",\n",
    "   \"\"\"如何使用linux 命令行去检查MA35D设备的系统状态？给我一个例子 \"\"\"\n",
    "\n",
    "   # \"\"\"\n",
    "   # explain following ffmpeg command\\n\n",
    "\n",
    "   # ffmpeg -y -hwaccel ama \\\n",
    "   #    -c:v h264_ama  -out_fmt nv12 -i <INPUT>  \\\n",
    "   #    -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; \\\n",
    "   #                   [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" \\\n",
    "   #    -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv \\\n",
    "   #    -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv  \\\n",
    "   #    -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv \\\n",
    "   #    -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
    "   # \"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine_beg.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "\n",
    "   # print(\"\")\n",
    "   # print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   # for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "   #    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are a Video ffmpeg & gstreamer technolodge expert.\\n\"\n",
    "    \"please answer the question in chinese\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge, please read the sources carefully.\\n\"\n",
    "    \"if the question is not releate with the RDMA, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a FPGA and RDMA expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_mode 有几种模式可选，refine， compact, tree_summarize 等，每一种都有对应的promopt template\n",
    "query_engine_tree_summarize = index.as_query_engine(response_mode='simple_summarize', streaming=True,similary_threshold=0.1, similarity_top_k=30)\n",
    "# query_engine.update_prompts(qa_template)  \n",
    "query_engine_tree_summarize.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": qa_template}\n",
    ")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "prompts_dict = query_engine_tree_summarize.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "   \"一个ma35D AV1 codec 能处理1080p的数据流最大到多少fps \"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_tree_summarize.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   response.print_response_stream()\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_refine = index.as_query_engine(response_mode='refine', similarity_top_k=100)\n",
    "\n",
    "template = (\n",
    "    \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"answer the question in chinese\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "   \"\"\"Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\"\"\",\n",
    "   \"\"\"Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\"\"\",\n",
    "   \"\"\" Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \"\"\",\n",
    "   \"\"\" Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "   \"\"\" Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "   # \"\"\"ffmpeg命令使用ma35d硬件转码, 用一条命令行使用split方式，将一个h264 4k60的文件同时转码成两个hevc和av1格式的文件,写出具体的命令行例子\"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_engine = index.as_query_engine()\n",
    "   query_response = query_engine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf \"hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4'\"\"\",\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_refine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{response.response}\")\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "   \n",
    "   \"how to enable ultra low latency for M35D Encoding?, give me a detailed ffmpge cmd example\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_tree_summarize.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   response.print_response_stream()\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
