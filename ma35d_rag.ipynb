{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 数量： 2\n",
      "GPU 0: Tesla V100-PCIE-16GB，内存使用情况：710.625 MB / 16384.0 MB\n",
      "GPU 1: Tesla V100-PCIE-16GB，内存使用情况：710.625 MB / 16384.0 MB\n"
     ]
    }
   ],
   "source": [
    "# %pip install nvidia-ml-py3\n",
    "import pynvml\n",
    "\n",
    "# 初始化 NVML 库\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 获取 GPU 数量\n",
    "num_gpus = pynvml.nvmlDeviceGetCount()\n",
    "print(\"GPU 数量：\", num_gpus)\n",
    "\n",
    "# 遍历每个 GPU，获取其资源信息\n",
    "for i in range(num_gpus):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    gpu_memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU {i}: {gpu_name.decode()}，内存使用情况：{gpu_memory_info.used / 1024 / 1024} MB / {gpu_memory_info.total / 1024 / 1024} MB\")\n",
    "\n",
    "# 关闭 NVML 库\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# LLamaIndex 使用 PyTorch 进行向量计算\n",
    "# 清理GPU 资源\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 数量： 2\n",
      "GPU 0: Tesla V100-PCIE-16GB，内存使用情况：710.625 MB / 16384.0 MB\n",
      "GPU 1: Tesla V100-PCIE-16GB，内存使用情况：710.625 MB / 16384.0 MB\n"
     ]
    }
   ],
   "source": [
    "# 初始化 NVML 库\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# 获取 GPU 数量\n",
    "num_gpus = pynvml.nvmlDeviceGetCount()\n",
    "print(\"GPU 数量：\", num_gpus)\n",
    "\n",
    "# 遍历每个 GPU，获取其资源信息\n",
    "for i in range(num_gpus):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    gpu_memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU {i}: {gpu_name.decode()}，内存使用情况：{gpu_memory_info.used / 1024 / 1024} MB / {gpu_memory_info.total / 1024 / 1024} MB\")\n",
    "\n",
    "# 关闭 NVML 库\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "04/19/2024 17:10:20 - [INFO] -sentence_transformers.SentenceTransformer->>>    Load pretrained SentenceTransformer: maidalun1020/bce-embedding-base_v1\n",
      "/home/xilinx/Documents/llamaindex_ma35_rag/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "04/19/2024 17:10:29 - [INFO] -sentence_transformers.SentenceTransformer->>>    2 prompts are loaded, with the keys: ['query', 'text']\n",
      "04/19/2024 17:10:33 - [INFO] -BCEmbedding.models.RerankerModel->>>    Loading from `maidalun1020/bce-reranker-base_v1`.\n",
      "04/19/2024 17:10:34 - [INFO] -BCEmbedding.models.RerankerModel->>>    Execute device: cuda;\t gpu num: 2;\t use fp16: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %pip install llama-index-llms-ollama\n",
    "# !pip install llama-index\n",
    "# %pip install llama-index-embeddings-ollama\n",
    "# %pip install docx2txt\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-instructor\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# load the ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from BCEmbedding.tools.llama_index import BCERerank\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# model set\n",
    "llama2_7b = \"llama2\"\n",
    "llama2_13b = \"llama2:13b\"\n",
    "llama3_8b = \"llama3\"\n",
    "llama3_70b = \"llama3:70b\"\n",
    "\n",
    "# connect with the ollama server, and talk with llama2 \n",
    "\n",
    "llm_llama = Ollama(model=llama3_8b, request_timeout=600, temperature=0.1, device='cuda') #base_url = 'http://localhost:11434',\n",
    "# llm_llama2 = Ollama(model=\"llama2:13b\", request_timeout=600, temperature=0.1) #base_url = 'http://localhost:11434',\n",
    "# embedding_model = OllamaEmbedding(model_name=\"llama2\",ollama_additional_kwargs={\"mirostat\": 0}) #base_url=\"http://localhost:11434\"\n",
    "\n",
    "embed_args = {'model_name': 'maidalun1020/bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 256, 'device': 'cuda'}\n",
    "embedding_model = HuggingFaceEmbedding(**embed_args)\n",
    "\n",
    "reranker_args = {'model': 'maidalun1020/bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda'}\n",
    "reranker_model = BCERerank(**reranker_args)\n",
    "\n",
    "Settings.llm = llm_llama\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "# Settings.num_output = 512\n",
    "# Settings.context_window = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:10:35 - [INFO] -chromadb.telemetry.product.posthog->>>    Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# create a vector storage\n",
    "# %pip install llama-index-vector-stores-chroma\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "chroma_client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bb4b86ed5747bfb485d820c086c39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c9524f757547639e33449e1a1b0ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935c51183f474bab9d31d2445c3c916e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caecd4da252149e087f8af0313425565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67263495339463fa4903e15989ce1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7df99e1920414f98bdc2327f3a887b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %pip install llama-index-readers-web\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/index.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/getting_started_on_prem.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/virtualization.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/tutorials.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/quality_analysis.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/ffmpeg/filters.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/tutorials.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/filters.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xcompositor.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/gstreamer/xabrladder.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/examples/xma/xma_apps.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/specs_and_features.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/package_feed.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/using_ffmpeg.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/using_gstreamer.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/unified_logging.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/tuning_video_quality.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/tuning_pipeline_latency.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/managing_compute_resources.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/c_apis.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/card_management.html\",\n",
    "     \"https://amd.github.io/ama-sdk/v1.1.2/encoder_comp_matrix.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-resampler.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-devices.html\",\n",
    "     \"https://ffmpeg.org/ffmpeg-all.html\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/H.264\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/H.265\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Encode/AV1\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Scaling\",\n",
    "     \"https://trac.ffmpeg.org/wiki/Null\",\n",
    "     \"https://trac.ffmpeg.org/wiki/FilteringGuide\",\n",
    "     ]\n",
    "     \n",
    ")\n",
    "\n",
    "collection_name = \"ma35_rag_base_beg\"\n",
    "collection = chroma_client.list_collections()\n",
    "if collection_name in collection:\n",
    "    chroma_client.delete_collection(collection_name)\n",
    "    chroma_client.clear_system_cache()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(docstore=documents, vector_store=vector_store)\n",
    "\n",
    "# 这个nodes 有什么用处\n",
    "from llama_index.core.node_parser import SimpleNodeParser \n",
    "# Initialize the parser \n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=500, chunk_overlap=20) \n",
    "# Parse documents into nodes \n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "# print(nodes[0])\n",
    "len(nodes)\n",
    "\n",
    "# %pip install ipywidgets\n",
    "# index = VectorStoreIndex.from_documents(documents,storage_context=storage_context,show_progress=True)\n",
    "index = VectorStoreIndex(nodes,embed_model=embedding_model,storage_context=storage_context,show_progress=True)\n",
    "\n",
    "# # documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # 读取 HTML 文件\n",
    "# with open(\"local_html/FFMPEG command line arguments - VideoDC - Xilinx Enterprise Wiki.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     html_content = file.read()\n",
    "\n",
    "# # 使用 Beautiful Soup 解析 HTML\n",
    "# soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# # 提取文档内容\n",
    "# document_text = soup.get_text()\n",
    "\n",
    "# # 打印文档内容\n",
    "# print(document_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your index from stored vectors\n",
    "\n",
    "collection_name = \"ma35_rag_base\"\n",
    "collection = chroma_client.list_collections()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, embed_model=embedding_model,storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = index.as_retriever()\n",
    "# relevant_docs = retriever.retrieve(\"what is the max transcode rate for 1080p30 stream\")\n",
    "# relevant_docs\n",
    "\n",
    "# \"\"\"\n",
    "# response_mode\n",
    "\n",
    "#     REFINE = \"refine\"\n",
    "#     COMPACT = \"compact\"\n",
    "#     SIMPLE_SUMMARIZE = \"simple_summarize\"\n",
    "#     TREE_SUMMARIZE = \"tree_summarize\"\n",
    "#     GENERATION = \"generation\"\n",
    "#     NO_TEXT = \"no_text\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# ResponseMode为tree_summarize时，LLM会对每一段文本进行最大长度的分割，并进行连续的读取和询问。这种模式的优点是可以保证对文本的完整理解和回答，但如果没有正确处理分割段落的情况，可能会导致错误的生成结果\n",
    "# ResponseMode为generation时，生成的回答不依赖于文档的内容，只基于提供的问题进行生成。这种模式适用于纯粹的问题回\n",
    "# ResponseMode为no_text时，生成的回答中不包含任何内容，仅作为占位符使用\n",
    "# ResponseMode为simple_summarize时，LLM会截取每段文本的相关句子（通常是第一句），并进行提炼生成回答。这种模式适用于对结果要求不高的场景。\n",
    "# ResponseMode为refine时，如果只有一个文本块（text_chunk），则会正常生成回答。但如果存在多个文本块，则会以类似轮询的方式迭代生成回答。这种模式可以对多个文本块进行迭代式的回答生成，逐步完善回答内容。\n",
    "# ResponseMode为compact时，生成的回答会将多个文本块（text_chunk）压缩到设定的最大长度，并生成一次回答。然后，根据后续内容对以往的答案进行改进和完善（即进行多次迭代）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine = index.as_query_engine(response_mode='simple_summarize')\n",
    "\n",
    "# template = (\n",
    "#     \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "#     \"Context information from multiple sources is below.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "#     \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "#     \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "#     \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "#     \"Answers need to be precise and concise.\\n\"\n",
    "#     \"Query: {query_str}\\n\"\n",
    "#     \"Answer: \"\n",
    "# )\n",
    "# qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "# query_engine.update_prompts(\n",
    "#     {\"response_synthesizer:text_qa_template\": qa_template}\n",
    "# )\n",
    "\n",
    "\n",
    "# template = (\n",
    "#     \"The original query is as follows: {query_str}.\\n\"\n",
    "#     \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "#     \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "#     \"-------------\\n\"\n",
    "#     \"{context_msg}\\n\"\n",
    "#     \"-------------\\n\"\n",
    "#     \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "#     \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "#     \"Answers need to be precise and concise.\\n\"\n",
    "#     \"Refined Answer: \"\n",
    "# )\n",
    "\n",
    "\n",
    "# qa_template = PromptTemplate(template)\n",
    "\n",
    "# query_engine.update_prompts(\n",
    "#     {\"response_synthesizer:refine_template\": qa_template}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7949f06ebaf45c4a4563a036645bb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command\n",
      "\n",
      "   \n",
      "   \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \n",
      "Answer:Empty Response\n",
      " \n",
      "source_nodes length:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here no database input, so no answer\n",
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command\\n\n",
    "   \n",
    "   \"ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf 'hwupload' -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4\" \"\"\",\n",
    "\n",
    "   # \"\"\"\n",
    "   # explain following ffmpeg command\\n\n",
    "\n",
    "   # ffmpeg -y -hwaccel ama \\\n",
    "   #    -c:v h264_ama  -out_fmt nv12 -i <INPUT>  \\\n",
    "   #    -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; \\\n",
    "   #                   [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" \\\n",
    "   #    -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv \\\n",
    "   #    -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv  \\\n",
    "   #    -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv \\\n",
    "   #    -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
    "   # \"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "   print(\" \")\n",
    "\n",
    "   \n",
    "   print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "      print(result)\n",
    "\n",
    "      # print(f\"Result {i}: Document ID {result['id']}, Title '{result['title']}', Similarity: {result.score}\")\n",
    "      print(f\"Result {i}\\n Similarity: {result.score}\\n content '{result.get_content()}\")\n",
    "\n",
    "   # print()\n",
    "   # # print(response.get_formatted_sources(length=10))\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"ma35_rag_base_beg\"\n",
    "collection = chroma_client.list_collections()\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, embed_model=embedding_model,storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_beg = index.as_query_engine(response_mode='refine',similarity_top_k=50, temperature=0.6,node_postprocessors=[reranker_model])\n",
    "\n",
    "template = (\n",
    "    \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"you must answer every question in chinese\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine_beg.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine_beg.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d60447acd649d4b962215a9585d41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/19/2024 17:12:18 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:12:23 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:12:28 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:12:35 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:12:41 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command 'ffmpeg -hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama -i infile.mp4 -c:v hevc_ama -b:v 1000K -r 60 -f mp4 -y transcoded.mp4' \n",
      "Answer:Based on the new context, I refine the original answer as follows:\n",
      "\n",
      "The FFmpeg command `ffmpeg -hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama -i infile.mp4 -c:v hevc_ama -b:v 1M -r 60 -f mp4 -y transcoded.mp4` is used to transcode the input video `infile.mp4` into H.265 (HEVC) format, using AMA hardware acceleration, and save the result in `transcoded.mp4`. The command also sets the video bitrate to 1,000 Kbps (1 Mbps), frame rate to 60 fps, and output file format to MP4.\n",
      "\n",
      "The main parameters explained are:\n",
      "\n",
      "* `-hwaccel ama`: Enables AMA hardware acceleration.\n",
      "* `-hwaccel_device /dev/ama_transcoder0`: Specifies the AMA hardware accelerator device as `/dev/ama_transcoder0`.\n",
      "* `-c:v h264_ama`: Encodes the input video into H.264 (AVC) format using AMA hardware acceleration.\n",
      "* `-i infile.mp4`: Specifies the input video file as `infile.mp4`.\n",
      "* `-c:v hevc_ama`: Encodes the output video into H.265 (HEVC) format using AMA hardware acceleration.\n",
      "* `-b:v 1M`: Sets the video bitrate to 1,000 Kbps (1 Mbps).\n",
      "* `-r 60`: Sets the frame rate to 60 fps.\n",
      "* `-f mp4`: Specifies the output file format as MP4.\n",
      "* `-y`: Forces overwrite of the existing output file.\n",
      "\n",
      "In summary, this command transcodes the input video into H.265 (HEVC) format using AMA hardware acceleration, while setting the video bitrate and frame rate.\n",
      "\n",
      "Note: The new context provides additional information on how to use FFmpeg for decoding and encoding only, which is not relevant to the original query. Therefore, I do not incorporate this information into the refined answer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67bd8483800416b814c7023f0c42c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:12:45 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:12:49 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:12:54 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:12:59 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:04 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question2: 详细解释一下下面的命令行 'gst-launch-1.0 filesrc location=<INPUT> ! parsebin ! h264parse ! ama_h264dec ! capsfilter 'caps=video/x-raw(memory:AMAMemory),format=NV12' !  ama_download ! filesink location=/tmp/h264.nv12'\n",
      "Answer:The refined answer:\n",
      "\n",
      "The command line `gst-launch-1.0 filesrc location=<INPUT> ! parsebin ! h264parse ! ama_h264dec ! capsfilter 'caps=video/x-raw(memory:AMAMemory),format=NV12' !  ama_download ! filesink location=/tmp/h264.nv12` is used to decode H.264 streams and save the decoded images as NV12 format in `/tmp/h264.nv12`. The pipeline consists of:\n",
      "1. `filesrc`: reads input files\n",
      "2. `parsebin`: unpacks containers to elementary streams\n",
      "3. `h264parse`: parses H.264 streams\n",
      "4. `ama_h264dec`: decodes H.264 streams and outputs NV12 format images\n",
      "5. `capsfilter`: specifies the output capabilities, requiring video/x-raw(memory:AMAMemory) format with NV12 format\n",
      "6. `ama_download`: downloads decoded images\n",
      "7. `filesink`: saves downloaded images to `/tmp/h264.nv12` files\n",
      "\n",
      "This command line is used for decoding H.264 streams and saving the decoded images as NV12 format.\n",
      "\n",
      "Additional information:\n",
      "\n",
      "* The `capsfilter` element is used to specify the output capabilities of the pipeline.\n",
      "* No changes are needed in the original answer as the new context provides additional details about the importance of disk speeds and tips for error checking and using fake sinks.\n",
      "\n",
      "Note: The context highlights the importance of disk speeds for operating on RAW files, recommends using a RAM disk (`/dev/shm`) and provides tips for error checking and using fake sinks to display performance numbers without writing outputs to disk.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a12c86241482089061b2dab11b6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:06 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:08 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:12 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:16 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:21 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question3: 如何使用linux 命令行去检查MA35D设备的系统状态？给我一个例子 \n",
      "Answer:To check the system status of an MA35D device using Linux command-line, you can use the `mautil` commands:\n",
      "\n",
      "1. Run `mautil examine` to get the status of the system and devices.\n",
      "2. Use `cat /sys/class/misc/ama_transcoder{x}/bus_id` to look up the PCIe bus ID of a device (where x is a number between 0 to total number of devices minus 1).\n",
      "3. Check the firmware version number using `cat /sys/class/misc/ama_transcoder0/version_information`.\n",
      "\n",
      "Additionally, you can use the following commands:\n",
      "\n",
      "* Run `mautil -d [<DBDF> | all] examine` to get detailed information about the system and device status.\n",
      "* Use `mautil -d [<DBDF> | all] validate` to validate the basic shell acceleration functionality.\n",
      "\n",
      "To check the current loading of all devices in your system, you can use the following command:\n",
      "\n",
      "```xrmadm /opt/amd/ama/ma35/scripts/list_cmd.json```\n",
      "\n",
      "This will generate a report in JSON format containing the load information for all compute unit (CU) resources. The report contains a section for each device in the system, including sub-sections for each CU (decoder, scaler, lookahead, encoder) in that device.\n",
      "\n",
      "Note: The `mautil` commands are specific to MA35D devices, so make sure you have the correct device before running these commands.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command 'ffmpeg -hwaccel ama -hwaccel_device /dev/ama_transcoder0 -c:v h264_ama -i infile.mp4 -c:v hevc_ama -b:v 1000K -r 60 -f mp4 -y transcoded.mp4' \"\"\",\n",
    "   \"\"\"详细解释一下下面的命令行 'gst-launch-1.0 filesrc location=<INPUT> ! parsebin ! h264parse ! ama_h264dec ! capsfilter 'caps=video/x-raw(memory:AMAMemory),format=NV12' !  ama_download ! filesink location=/tmp/h264.nv12'\"\"\",\n",
    "   \"\"\"如何使用linux 命令行去检查MA35D设备的系统状态？给我一个例子 \"\"\"\n",
    "\n",
    "   # \"\"\"\n",
    "   # explain following ffmpeg command\\n\n",
    "\n",
    "   # ffmpeg -y -hwaccel ama \\\n",
    "   #    -c:v h264_ama  -out_fmt nv12 -i <INPUT>  \\\n",
    "   #    -filter_complex \"scaler_ama=outputs=4:out_res=(1920x1080|full|nv12)(1280x720|full|nv12)(720x480|full|nv12)(360x240|full|nv12) [a][b][c][d]; \\\n",
    "   #                   [a]hwdownload,format=nv12[a1];[b]hwdownload,format=nv12[b1];[c]hwdownload,format=nv12[c1];[d]hwdownload,format=nv12[d1]\" \\\n",
    "   #    -map '[a1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_1080p.yuv \\\n",
    "   #    -map '[b1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_720p.yuv  \\\n",
    "   #    -map '[c1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_480p.yuv \\\n",
    "   #    -map '[d1]' -f rawvideo -pix_fmt nv12 -y /tmp/scale_240p.yuv\n",
    "   # \"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_response = query_engine_beg.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n",
    "\n",
    "   # print(\"\")\n",
    "   # print(f\"source_nodes length:{len(query_response.source_nodes)}\")\n",
    "   # for i, result in enumerate(query_response.source_nodes, start=1):\n",
    "   #    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are a Video ffmpeg & gstreamer technolodge expert.\\n\"\n",
    "    \"please answer the question in chinese\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge, please read the sources carefully.\\n\"\n",
    "    \"if the question is not releate with the RDMA, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a FPGA and RDMA expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response_mode 有几种模式可选，refine， compact, tree_summarize 等，每一种都有对应的promopt template\n",
    "query_engine_tree_summarize = index.as_query_engine(response_mode='simple_summarize', streaming=True,similary_threshold=0.1, similarity_top_k=30)\n",
    "# query_engine.update_prompts(qa_template)  \n",
    "query_engine_tree_summarize.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": qa_template}\n",
    ")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "prompts_dict = query_engine_tree_summarize.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58902aa0a9b462d82a594c2da57a1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: 一个ma35D AV1 codec 能处理1080p的数据流最大到多少fps \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:24 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, I can help you with your question.\n",
      "\n",
      "According to the text, the MA35D accelerator card has Video Codec Unit (VCU) cores that support AOM AV1: AOMedia Video 1 - Main, High up to Level 5.3.\n",
      "\n",
      "The text also mentions that the VCU cores support resolutions from 128x128 to 3840x2160 portrait and landscape.\n",
      "\n",
      "However, it does not explicitly mention the maximum frame rate (fps) for a specific resolution like 1080p.\n",
      "\n",
      "To answer your question, I would need more information about the MA35D AV1 codec's capabilities or specifications. Unfortunately, this context does not provide that information.\n",
      "\n",
      "If you have any additional context or specifications about the MA35D AV1 codec, I'd be happy to help you with your query!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"一个ma35D AV1 codec 能处理1080p的数据流最大到多少fps \"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_tree_summarize.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   response.print_response_stream()\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "query_engine_refine = index.as_query_engine(response_mode='refine', similarity_top_k=100)\n",
    "\n",
    "template = (\n",
    "    \"You are video transcode expert and very faimilay with ffmpge.\\n\"\n",
    "    \"Context information from multiple sources is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"please read the above context information carefully. and anwer the question.\\n\"\n",
    "    \"if the question is not releate with video process, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am a video expert.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"answer the question in chinese\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ccece6cadd47199b74303d62010ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:30 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\n",
      "Answer:To use FFmpeg decoder to decode a clip that is already encoded in H.264 and save it to disk in a RAW format, you can use the following command:\n",
      "\n",
      "ffmpeg -i input.mp4 -c:v rawvideo -f rawvideo output.raw\n",
      "\n",
      "This will read the input file (input.mp4), decode the H.264 video stream using FFmpeg's built-in decoder, and write the decoded frames to a RAW file (output.raw) in a format that can be read by other applications.\n",
      "\n",
      "Note: The `-c:v` option specifies the codec for the output video stream, and `rawvideo` is the codec for a raw, uncompressed video stream. The `-f` option specifies the format of the output file, which in this case is `rawvideo`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4890a334d6194b9195cf1dd024ecdb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:32 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question2: Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\n",
      "Answer:Here is the answer:\n",
      "\n",
      "ffmpeg -hwaccel ama -i <INPUT> -vf \"format=yuv420p, hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y output.mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196a4577dc0349178bf108e4a37e76c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:33 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question3:  Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \n",
      "Answer:To encode YUV 4:2:2 10-bit pixel format to YUV 4:2:0 8-bit, you can use the following command:\n",
      "\n",
      "ffmpeg -hwaccel ama -i <INPUT> -vf \"format=yuv420p, hwupload\" -c:v h264_ama -b:v 1M <OUTPUT>\n",
      "\n",
      "This command uses the `format` filter to convert the input YUV 4:2:2 10-bit pixel format to YUV 4:2:0 8-bit. The rest of the flags are used to specify the output video codec and bitrate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b6d624710042f9a59dece41f926a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:36 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question4:  Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\n",
      "Answer:To scale the decoded video into different resolutions (10, 720, 480, and 240) using ffmpeg, you can use the scaler filter. Here's an example command:\n",
      "\n",
      "ffmpeg -i input.h264 -filter_complex \"scale=w=1080:h=720[720p];scale=w=640:h=480[480p];scale=w=384:h=240[240p]\" -vsync 0 -c:v rawvideo -f rawvideo output_10.80p.raw output_720p.raw output_480p.raw output_240p.raw\n",
      "\n",
      "This command will decode the input H.264 file, scale it to four different resolutions (1080x720, 640x480, and 384x240), and save each resolution as a separate RAW video file.\n",
      "\n",
      "Note that you can adjust the scaling parameters (w and h) to achieve the desired resolutions. Also, make sure to specify the correct codec and format for your output files.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0893d01950f843fcbcc23ff8cda08843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:39 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question5:  Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\n",
      "Answer:Here is a command that uses FFmpeg's `overlay` filter to decode an existing H.264 file, scale it into 1080p, 720p, 480p, and 240p four resolutions, and save the raw outputs to disk:\n",
      "\n",
      "ffmpeg -i <INPUT> -filter_complex \"scale=10:80,format=yuv420p[00];scale=7:20,format=yuv420p[11];scale=4:80,format=yuv420p[22];scale=2:40,format=yuv420p[33];[00][11]hstack[top];[22][33]hstack[bot]; [top][bot] vstack\" -c:v raw_yuv -f raw <OUT DIR>/output_%04d.raw\n",
      "\n",
      "This command will generate four files: output_1080.raw, output_720.raw, output_480.raw, and output_240.raw.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"\"\"Using ffmpeg Decoder a clip that is already encoded in H.264, and will decode the file into a RAW format and save it to disk.\"\"\",\n",
    "   \"\"\"Using ffmpeg encode a RAW 1080p60 clip in YUV420 format. Pass the clip to the MA35D encoder to produce an AV1 encoded MP4 output with a target bitrate of 5Mbps and saves it to disk. please do not use -re option\"\"\",\n",
    "   \"\"\" Using ffmpeg do the Bit Conversion, To encode YUV 4:2:2 10 bit pixel format to YUV 4:2:0 8 bit format' \"\"\",\n",
    "   \"\"\" Using ffmpeg decodes an existing H.264 file and then scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "   \"\"\" Using ffmpeg one cmd line, decodes an existing H.264 file and then using scaler_ama scales it into 1080p/720p/480p/240p four resolutions and save the RAW outputs to disk under\"\"\",\n",
    "   # \"\"\"ffmpeg命令使用ma35d硬件转码, 用一条命令行使用split方式，将一个h264 4k60的文件同时转码成两个hevc和av1格式的文件,写出具体的命令行例子\"\"\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   query_engine = index.as_query_engine()\n",
    "   query_response = query_engine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{query_response.response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ed2c45f3794a8098adc502e8b71e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:13:42 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:45 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:48 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:52 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:55 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:13:58 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:14:01 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:14:04 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:14:06 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:14:09 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:14:11 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "04/19/2024 17:14:14 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: explain following ffmpeg command ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf \"hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4'\n",
      "Answer:Here's a rewritten answer using the new context:\n",
      "\n",
      "This FFmpeg command demonstrates how to process video streams with AMA hardware acceleration. The command starts by specifying the input format as `rawvideo`, resolution as `19x20x10x80`, and frame rate as `24`. The input file is named `cut1_10p.nv12`. The `-vf \"hwupload\"` option instructs FFmpeg to upload the input video data to the hardware accelerator for processing. The codec used for encoding is AV1 with hardware acceleration (`-c:av av1_ama`), and the target bitrate is set to 5 megabits per second (`-b:v 5M`).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \"\"\"explain following ffmpeg command ffmpeg -hwaccel ama -f rawvideo -s 1920x1080 -framerate 24 -i cut1_1080p.nv12 -vf \"hwupload\" -c:v av1_ama -b:v 5M -f mp4 -y 1.av1_1080p_1.mp4'\"\"\",\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_refine.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   print(f\"Answer:{response.response}\")\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152ebdf5f9c34806904aa444cf1eaa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: how to enable ultra low latency for M35D Encoding?, give me a detailed ffmpge cmd example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2024 17:14:17 - [INFO] -httpx->>>    HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, it appears that you are looking for instructions on how to enable ultra-low latency for M35D encoding using FFmpeg.\n",
      "\n",
      "According to the context information, the command to enable ultra-low latency is:\n",
      "\n",
      "`ma35_transcoder_app -streams 1 -frames 2,000 -c:v av1_ama -b:v 10M -latency_mode 2 -o h264_av1_transcode.av1`\n",
      "\n",
      "This command uses the `ma35_transcoder_app` application to transcode an input file using the H.264 to AV1 format. The `-streams 1` option specifies that only one stream should be processed, and the `-frames 2,000` option sets the maximum number of frames to process.\n",
      "\n",
      "The `-c:v av1_ama` option specifies the video codec as AV1 with AMA (Advanced Media Acceleration) support. The `-b:v 10M` option sets the bitrate for the video stream to 10 megabits per second.\n",
      "\n",
      "The `-latency_mode 2` option enables ultra-low latency mode, which is intended for real-time encoding applications that require low latency and high throughput.\n",
      "\n",
      "Finally, the `-o h264_av1_transcode.av1` option specifies the output file name as `h264_av1_transcode.av1`.\n",
      "\n",
      "Please note that this command may not work without additional configuration or setup specific to your system and use case.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "   \n",
    "   \"how to enable ultra low latency for M35D Encoding?, give me a detailed ffmpge cmd example\"\n",
    "]\n",
    "\n",
    "counter = 0\n",
    "for question in questions:\n",
    "   counter = counter + 1\n",
    "   response = query_engine_tree_summarize.query(question)\n",
    "   print(f\"Question{counter}: {question}\")\n",
    "   response.print_response_stream()\n",
    "   print()\n",
    "   # print(response.get_formatted_sources(length=10))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
